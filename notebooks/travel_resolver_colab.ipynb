{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee25b6d1",
   "metadata": {},
   "source": [
    "# Travel Intent Classification & NER - Google Colab GPU\n",
    "\n",
    "Ce notebook Colab (optimisé pour **GPU**) utilise **Transformers** pour du **Fine-Tuning** sur :\n",
    "1. **Intent Classification** : Classification en 4 classes (TRIP, NOT_TRIP, UNKNOWN, NOT_FRENCH)\n",
    "2. **Named Entity Recognition (NER)** : Extraction de Departure et Destination\n",
    "\n",
    "**Dataset** : Chargé depuis Google Drive (`/content/drive/MyDrive/nlp_dataset/`)\n",
    "\n",
    "**Modèle** : CamemBERT (modèle français pré-entraîné)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c497b897",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Travel Order Resolver - Google Colab GPU + Transformers Fine-Tuning')\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9c9b9f",
   "metadata": {},
   "source": [
    "## 1 - Vérification du GPU et Installation des dépendances\n",
    "\n",
    "Vérifie la disponibilité du GPU et installe les librairies nécessaires :\n",
    "- `transformers` : Pour CamemBERT et le fine-tuning\n",
    "- `datasets` : Pour gérer les datasets\n",
    "- `evaluate` : Pour les métriques d'évaluation\n",
    "- `seqeval` : Pour les métriques NER\n",
    "- `accelerate` : Pour l'optimisation GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fffb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier la disponibilité du GPU\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"Pas de GPU détecté. Allez dans Runtime > Change runtime type > GPU\")\n",
    "\n",
    "# Installation des packages\n",
    "!pip install -q transformers datasets evaluate seqeval accelerate\n",
    "print(\"Installation terminée!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb79d1a6",
   "metadata": {},
   "source": [
    "## 2 - Montage de Google Drive et Chargement des Datasets\n",
    "\n",
    "Monte Google Drive pour accéder aux datasets depuis `/content/drive/MyDrive/nlp_dataset/`\n",
    "\n",
    "**Structure attendue dans Drive** :\n",
    "```\n",
    "MyDrive/\n",
    "  nlp_dataset/\n",
    "    train_set.csv\n",
    "    test_set.csv\n",
    "    cities_fr.txt (optionnel)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87de4972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monter Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Chemins vers les datasets dans Google Drive\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "DATASET_DIR = '/content/drive/MyDrive/nlp_dataset'\n",
    "train_csv = os.path.join(DATASET_DIR, 'train_set.csv')\n",
    "test_csv = os.path.join(DATASET_DIR, 'test_set.csv')\n",
    "cities_file = os.path.join(DATASET_DIR, 'cities_fr.txt')\n",
    "\n",
    "# Vérifier l'existence des fichiers\n",
    "if not os.path.exists(train_csv) or not os.path.exists(test_csv):\n",
    "    print(\"ERREUR: Fichiers introuvables!\")\n",
    "    print(f\"   Assurez-vous que les fichiers sont dans: {DATASET_DIR}\")\n",
    "    print(\"   - train_set.csv\")\n",
    "    print(\"   - test_set.csv\")\n",
    "else:\n",
    "    print(\" Fichiers trouvés!\")\n",
    "    train_df = pd.read_csv(train_csv, encoding='utf-8')\n",
    "    test_df = pd.read_csv(test_csv, encoding='utf-8')\n",
    "    print(f\"\\n Train shape: {train_df.shape}\")\n",
    "    print(f\" Test shape: {test_df.shape}\")\n",
    "    print(f\"\\n Distribution des classes (Train):\")\n",
    "    print(train_df['intent'].value_counts())\n",
    "    print(f\"\\n Aperçu du dataset:\")\n",
    "    display(train_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fd1ddd",
   "metadata": {},
   "source": [
    "## 3 - Prétraitement : Parser les Entities JSON\n",
    "\n",
    "Parse la colonne `entities` (format JSON) pour extraire les annotations Departure/Destination.\n",
    "\n",
    "**Format des entities** :\n",
    "```json\n",
    "[{\"start\": 10, \"end\": 15, \"label\": \"Departure\"}, {\"start\": 20, \"end\": 25, \"label\": \"Destination\"}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659b0858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def parse_entities_field(row):\n",
    "    \"\"\"Parse la colonne entities (JSON) et valide les annotations.\"\"\"\n",
    "    try:\n",
    "        ents = json.loads(row['entities'])\n",
    "    except Exception:\n",
    "        ents = []\n",
    "    \n",
    "    valid = []\n",
    "    txt = row.get('text', '')\n",
    "    for ent in ents:\n",
    "        if 'start' in ent and 'end' in ent and 'label' in ent:\n",
    "            if 0 <= ent['start'] < ent['end'] <= len(txt):\n",
    "                valid.append(ent)\n",
    "    return valid\n",
    "\n",
    "# Appliquer le parsing\n",
    "train_df['parsed_entities'] = train_df.apply(parse_entities_field, axis=1)\n",
    "test_df['parsed_entities'] = test_df.apply(parse_entities_field, axis=1)\n",
    "\n",
    "print(\"Entities parsées avec succès!\")\n",
    "print(f\"\\nExemple avec entities:\")\n",
    "trip_examples = train_df[train_df['intent'] == 'TRIP'].head(2)\n",
    "for idx, row in trip_examples.iterrows():\n",
    "    print(f\"\\nTexte: {row['text']}\")\n",
    "    print(f\"Intent: {row['intent']}\")\n",
    "    print(f\"Entities: {row['parsed_entities']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd48f7c",
   "metadata": {},
   "source": [
    "## 4 - Préparation des Datasets HuggingFace\n",
    "\n",
    "Conversion des DataFrames Pandas en Datasets HuggingFace pour l'entraînement avec Transformers.\n",
    "\n",
    "Les datasets seront préparés pour :\n",
    "1. **Intent Classification** : Text → Label (TRIP, NOT_TRIP, UNKNOWN, NOT_FRENCH)\n",
    "2. **NER** : Text → Tokens avec labels BIO (B-Departure, I-Departure, B-Destination, I-Destination, O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb4df48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# === DATASET POUR INTENT CLASSIFICATION ===\n",
    "\n",
    "# Encoder les labels d'intent\n",
    "label_encoder = LabelEncoder()\n",
    "all_intents = list(train_df['intent'].unique()) + list(test_df['intent'].unique())\n",
    "label_encoder.fit(list(set(all_intents)))\n",
    "\n",
    "print(\"Classes d'intent:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"   {i}: {label}\")\n",
    "\n",
    "# Créer les datasets HuggingFace\n",
    "train_df['label'] = label_encoder.transform(train_df['intent'])\n",
    "test_df['label'] = label_encoder.transform(test_df['intent'])\n",
    "\n",
    "intent_train_dataset = Dataset.from_pandas(train_df[['text', 'label']])\n",
    "intent_test_dataset = Dataset.from_pandas(test_df[['text', 'label']])\n",
    "\n",
    "print(f\"\\nIntent datasets créés:\")\n",
    "print(f\"   Train: {len(intent_train_dataset)} exemples\")\n",
    "print(f\"   Test: {len(intent_test_dataset)} exemples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8129008d",
   "metadata": {},
   "source": [
    "## 5 - Fine-Tuning Intent Classification avec CamemBERT\n",
    "\n",
    "Entraînement d'un modèle de classification d'intent basé sur **CamemBERT** (modèle français).\n",
    "\n",
    "**Paramètres** :\n",
    "- Modèle : `camembert-base`\n",
    "- Nombre d'epochs : 3\n",
    "- Batch size : 16 (train) / 32 (eval)\n",
    "- Learning rate : 2e-5\n",
    "- Optimisation : AdamW avec weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efd4bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Désactiver WandB (tracking des expériences)\n",
    "import os\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "print(\"✅ WandB désactivé\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7586d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Charger le tokenizer CamemBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained('camembert-base')\n",
    "\n",
    "# Fonction de tokenization\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# Tokenizer les datasets\n",
    "tokenized_train = intent_train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = intent_test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Charger le modèle CamemBERT pour classification\n",
    "num_labels = len(label_encoder.classes_)\n",
    "intent_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'camembert-base',\n",
    "    num_labels=num_labels,\n",
    "    id2label={i: label for i, label in enumerate(label_encoder.classes_)},\n",
    "    label2id={label: i for i, label in enumerate(label_encoder.classes_)}\n",
    ")\n",
    "\n",
    "# Définir les métriques\n",
    "accuracy_metric = evaluate.load('accuracy')\n",
    "f1_metric = evaluate.load('f1')\n",
    "\n",
    "def compute_intent_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='macro')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy['accuracy'],\n",
    "        'f1_macro': f1['f1']\n",
    "    }\n",
    "\n",
    "# Arguments d'entraînement (optimisés pour GPU)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/content/drive/MyDrive/nlp_dataset/models/intent_classifier',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1_macro',\n",
    "    logging_steps=50,\n",
    "    warmup_steps=100,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    push_to_hub=False,\n",
    "    report_to='none',  # Désactiver WandB et autres trackers\n",
    ")\n",
    "\n",
    "# Créer le Trainer\n",
    "intent_trainer = Trainer(\n",
    "    model=intent_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_intent_metrics\n",
    ")\n",
    "\n",
    "print(\"🚀 Début du fine-tuning Intent Classification...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Entraîner le modèle\n",
    "intent_trainer.train()\n",
    "\n",
    "print(\"\\n✅ Fine-tuning Intent Classification terminé!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd00c5d",
   "metadata": {},
   "source": [
    "## 6 - Évaluation du Modèle Intent Classification\n",
    "\n",
    "Évalue le modèle entraîné sur le test set et affiche les métriques détaillées :\n",
    "- **Accuracy** : Précision globale\n",
    "- **F1-Score (Macro)** : F1 moyen sur toutes les classes\n",
    "- **Matrice de confusion** : Visualisation des erreurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfcc747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Évaluer sur le test set\n",
    "eval_results = intent_trainer.evaluate()\n",
    "\n",
    "print(\"Résultats sur le Test Set:\")\n",
    "print(\"=\"*70)\n",
    "for key, value in eval_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"   {key}: {value:.4f}\")\n",
    "\n",
    "# Prédictions détaillées\n",
    "predictions = intent_trainer.predict(tokenized_test)\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=-1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(\n",
    "    true_labels,\n",
    "    predicted_labels,\n",
    "    target_names=label_encoder.classes_,\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=label_encoder.classes_,\n",
    "    yticklabels=label_encoder.classes_\n",
    ")\n",
    "plt.title('Matrice de Confusion - Intent Classification')\n",
    "plt.ylabel('Vraie Classe')\n",
    "plt.xlabel('Classe Prédite')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Sauvegarder le meilleur modèle\n",
    "intent_trainer.save_model('/content/drive/MyDrive/nlp_dataset/models/intent_classifier_best')\n",
    "tokenizer.save_pretrained('/content/drive/MyDrive/nlp_dataset/models/intent_classifier_best')\n",
    "\n",
    "print(\"\\nModèle sauvegardé dans Google Drive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b6c7fb",
   "metadata": {},
   "source": [
    "## 7 - Préparation du Dataset NER (Token Classification)\n",
    "\n",
    "Conversion des annotations au format **BIO** (Begin-Inside-Outside) pour le NER :\n",
    "- **O** : Pas une entité\n",
    "- **B-Departure** : Début du lieu de départ\n",
    "- **I-Departure** : Continuation du lieu de départ\n",
    "- **B-Destination** : Début du lieu de destination\n",
    "- **I-Destination** : Continuation du lieu de destination\n",
    "\n",
    "Seuls les exemples avec `intent=TRIP` ont des annotations NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b23c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Tokenizer rapide pour NER (nécessaire pour offset_mapping)\n",
    "tokenizer_fast = AutoTokenizer.from_pretrained('camembert-base', use_fast=True)\n",
    "\n",
    "# Labels NER en format BIO\n",
    "ner_labels = ['O', 'B-Departure', 'I-Departure', 'B-Destination', 'I-Destination']\n",
    "label2id = {label: i for i, label in enumerate(ner_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(\"Labels NER:\")\n",
    "for label_id, label_name in id2label.items():\n",
    "    print(f\"   {label_id}: {label_name}\")\n",
    "\n",
    "def convert_to_bio_tags(text, entities):\n",
    "    \"\"\"\n",
    "    Convertit les annotations (start, end, label) en tags BIO pour chaque token.\n",
    "    \"\"\"\n",
    "    # Tokenizer le texte\n",
    "    encoding = tokenizer_fast(\n",
    "        text,\n",
    "        return_offsets_mapping=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    offset_mapping = encoding['offset_mapping']\n",
    "    labels = ['O'] * len(offset_mapping)\n",
    "    \n",
    "    # Pour chaque entité, marquer les tokens correspondants\n",
    "    for entity in entities:\n",
    "        start_char = entity['start']\n",
    "        end_char = entity['end']\n",
    "        entity_label = entity['label']\n",
    "        \n",
    "        token_indices = []\n",
    "        for idx, (token_start, token_end) in enumerate(offset_mapping):\n",
    "            # Ignorer les tokens padding\n",
    "            if token_start == token_end == 0:\n",
    "                continue\n",
    "            \n",
    "            # Vérifier si le token chevauche l'entité\n",
    "            if not (token_end <= start_char or token_start >= end_char):\n",
    "                token_indices.append(idx)\n",
    "        \n",
    "        # Appliquer les tags BIO\n",
    "        if token_indices:\n",
    "            labels[token_indices[0]] = f'B-{entity_label}'\n",
    "            for idx in token_indices[1:]:\n",
    "                labels[idx] = f'I-{entity_label}'\n",
    "    \n",
    "    # Convertir en IDs\n",
    "    label_ids = [label2id.get(label, 0) for label in labels]\n",
    "    \n",
    "    return {\n",
    "        'input_ids': encoding['input_ids'],\n",
    "        'attention_mask': encoding['attention_mask'],\n",
    "        'labels': label_ids\n",
    "    }\n",
    "\n",
    "# Préparer les datasets NER\n",
    "print(\"\\nConversion en format BIO...\")\n",
    "ner_train_data = []\n",
    "for _, row in train_df.iterrows():\n",
    "    entities = row['parsed_entities'] if row['intent'] == 'TRIP' else []\n",
    "    ner_example = convert_to_bio_tags(row['text'], entities)\n",
    "    ner_train_data.append(ner_example)\n",
    "\n",
    "ner_test_data = []\n",
    "for _, row in test_df.iterrows():\n",
    "    entities = row['parsed_entities'] if row['intent'] == 'TRIP' else []\n",
    "    ner_example = convert_to_bio_tags(row['text'], entities)\n",
    "    ner_test_data.append(ner_example)\n",
    "\n",
    "# Créer les datasets HuggingFace\n",
    "ner_train_dataset = Dataset.from_list(ner_train_data)\n",
    "ner_test_dataset = Dataset.from_list(ner_test_data)\n",
    "\n",
    "print(f\"Datasets NER créés:\")\n",
    "print(f\"   Train: {len(ner_train_dataset)} exemples\")\n",
    "print(f\"   Test: {len(ner_test_dataset)} exemples\")\n",
    "\n",
    "# Afficher un exemple\n",
    "print(f\"\\nExemple de tokenization BIO:\")\n",
    "example_idx = train_df[train_df['intent'] == 'TRIP'].index[0]\n",
    "example_text = train_df.loc[example_idx, 'text']\n",
    "example_entities = train_df.loc[example_idx, 'parsed_entities']\n",
    "print(f\"Texte: {example_text}\")\n",
    "print(f\"Entities: {example_entities}\")\n",
    "\n",
    "tokens = tokenizer_fast.tokenize(example_text)\n",
    "bio_example = convert_to_bio_tags(example_text, example_entities)\n",
    "bio_labels = [id2label[lid] for lid in bio_example['labels']]\n",
    "\n",
    "print(f\"\\nTokens et labels BIO (premiers 15):\")\n",
    "for token, label in list(zip(tokens, bio_labels))[:15]:\n",
    "    print(f\"   {token:20s} → {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bea529",
   "metadata": {},
   "source": [
    "## 8 - Fine-Tuning NER avec CamemBERT (Token Classification)\n",
    "\n",
    "Entraînement d'un modèle de **Named Entity Recognition** basé sur CamemBERT.\n",
    "\n",
    "**Tâche** : Identifier et extraire les lieux de départ (Departure) et de destination (Destination) dans les phrases de voyage.\n",
    "\n",
    "**Paramètres** :\n",
    "- Modèle : `camembert-base` (Token Classification)\n",
    "- Nombre d'epochs : 4\n",
    "- Batch size : 8 (train) / 16 (eval)\n",
    "- Learning rate : 3e-5\n",
    "- Métrique : F1-Score (seqeval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e0589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Charger le modèle CamemBERT pour Token Classification\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    'camembert-base',\n",
    "    num_labels=len(ner_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Data collator pour padding dynamique\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer_fast)\n",
    "\n",
    "# Charger la métrique seqeval\n",
    "seqeval_metric = evaluate.load('seqeval')\n",
    "\n",
    "def compute_ner_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Calcule les métriques NER (precision, recall, F1) avec seqeval.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Convertir les prédictions et labels en listes de tags\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    for pred_seq, label_seq in zip(predictions, labels):\n",
    "        true_tags = []\n",
    "        pred_tags = []\n",
    "        \n",
    "        for pred_id, label_id in zip(pred_seq, label_seq):\n",
    "            # Ignorer les labels de padding (-100)\n",
    "            if label_id != -100:\n",
    "                true_tags.append(id2label[label_id])\n",
    "                pred_tags.append(id2label[pred_id])\n",
    "        \n",
    "        true_labels.append(true_tags)\n",
    "        pred_labels.append(pred_tags)\n",
    "    \n",
    "    # Calculer les métriques\n",
    "    results = seqeval_metric.compute(predictions=pred_labels, references=true_labels)\n",
    "    \n",
    "    return {\n",
    "        'precision': results['overall_precision'],\n",
    "        'recall': results['overall_recall'],\n",
    "        'f1': results['overall_f1'],\n",
    "        'accuracy': results['overall_accuracy']\n",
    "    }\n",
    "\n",
    "# Arguments d'entraînement (optimisés pour GPU)\n",
    "ner_training_args = TrainingArguments(\n",
    "    output_dir='/content/drive/MyDrive/nlp_dataset/models/ner_model',\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    logging_steps=100,\n",
    "    warmup_steps=200,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    push_to_hub=False,\n",
    "    report_to='none',  # Désactiver WandB et autres trackers\n",
    ")\n",
    "\n",
    "# Créer le Trainer\n",
    "ner_trainer = Trainer(\n",
    "    model=ner_model,\n",
    "    args=ner_training_args,\n",
    "    train_dataset=ner_train_dataset,\n",
    "    eval_dataset=ner_test_dataset,\n",
    "    tokenizer=tokenizer_fast,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_ner_metrics\n",
    ")\n",
    "\n",
    "print(\"🚀 Début du fine-tuning NER (Token Classification)...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Entraîner le modèle\n",
    "ner_trainer.train()\n",
    "\n",
    "print(\"\\n✅ Fine-tuning NER terminé!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96f00ff",
   "metadata": {},
   "source": [
    "## 9 - Évaluation du Modèle NER\n",
    "\n",
    "Évalue le modèle NER sur le test set avec des métriques détaillées par entité :\n",
    "- **Precision** : Précision de détection des entités\n",
    "- **Recall** : Taux de rappel des entités\n",
    "- **F1-Score** : Score F1 global et par type d'entité (Departure, Destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45470ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report as seqeval_report\n",
    "\n",
    "# Évaluer sur le test set\n",
    "ner_eval_results = ner_trainer.evaluate()\n",
    "\n",
    "print(\"Résultats NER sur le Test Set:\")\n",
    "print(\"=\"*70)\n",
    "for key, value in ner_eval_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"   {key}: {value:.4f}\")\n",
    "\n",
    "# Prédictions détaillées\n",
    "ner_predictions = ner_trainer.predict(ner_test_dataset)\n",
    "predicted_logits = ner_predictions.predictions\n",
    "predicted_labels = np.argmax(predicted_logits, axis=-1)\n",
    "true_labels = ner_predictions.label_ids\n",
    "\n",
    "# Convertir en tags pour seqeval\n",
    "true_tags_list = []\n",
    "pred_tags_list = []\n",
    "\n",
    "for pred_seq, label_seq in zip(predicted_labels, true_labels):\n",
    "    true_tags = []\n",
    "    pred_tags = []\n",
    "    \n",
    "    for pred_id, label_id in zip(pred_seq, label_seq):\n",
    "        if label_id != -100:\n",
    "            true_tags.append(id2label[label_id])\n",
    "            pred_tags.append(id2label[pred_id])\n",
    "    \n",
    "    true_tags_list.append(true_tags)\n",
    "    pred_tags_list.append(pred_tags)\n",
    "\n",
    "# Classification report détaillé\n",
    "print(\"\\nClassification Report NER (par entité):\")\n",
    "print(\"=\"*70)\n",
    "print(seqeval_report(true_tags_list, pred_tags_list, digits=4))\n",
    "\n",
    "# Sauvegarder le meilleur modèle\n",
    "ner_trainer.save_model('/content/drive/MyDrive/nlp_dataset/models/ner_model_best')\n",
    "tokenizer_fast.save_pretrained('/content/drive/MyDrive/nlp_dataset/models/ner_model_best')\n",
    "\n",
    "print(\"\\nModèle NER sauvegardé dans Google Drive!\")\n",
    "\n",
    "# Afficher quelques exemples de prédictions\n",
    "print(\"\\nExemples de prédictions NER:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "trip_test_indices = test_df[test_df['intent'] == 'TRIP'].head(5).index\n",
    "for idx in trip_test_indices:\n",
    "    text = test_df.loc[idx, 'text']\n",
    "    true_entities = test_df.loc[idx, 'parsed_entities']\n",
    "    \n",
    "    # Prédire avec le modèle\n",
    "    inputs = tokenizer_fast(text, return_tensors='pt', truncation=True, max_length=128)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        ner_model.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = ner_model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    # Décoder les tokens et labels\n",
    "    tokens = tokenizer_fast.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    predicted_tags = [id2label[p.item()] for p in predictions[0]]\n",
    "    \n",
    "    print(f\"\\nTexte: {text}\")\n",
    "    print(f\"Vraies entités: {true_entities}\")\n",
    "    print(\"Prédictions:\")\n",
    "    \n",
    "    current_entity = None\n",
    "    current_tokens = []\n",
    "    \n",
    "    for token, tag in zip(tokens, predicted_tags):\n",
    "        if token in ['<s>', '</s>', '<pad>']:\n",
    "            continue\n",
    "        \n",
    "        if tag.startswith('B-'):\n",
    "            if current_entity:\n",
    "                print(f\"   {current_entity}: {''.join(current_tokens).replace('▁', ' ').strip()}\")\n",
    "            current_entity = tag[2:]\n",
    "            current_tokens = [token]\n",
    "        elif tag.startswith('I-') and current_entity:\n",
    "            current_tokens.append(token)\n",
    "        else:\n",
    "            if current_entity:\n",
    "                print(f\"   {current_entity}: {''.join(current_tokens).replace('▁', ' ').strip()}\")\n",
    "                current_entity = None\n",
    "                current_tokens = []\n",
    "    \n",
    "    if current_entity:\n",
    "        print(f\"   {current_entity}: {''.join(current_tokens).replace('▁', ' ').strip()}\")\n",
    "    \n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dbecb5",
   "metadata": {},
   "source": [
    "## 10 - Pipeline d'Inférence Complet\n",
    "\n",
    "Combine les deux modèles (Intent Classification + NER) pour créer un pipeline d'inférence complet.\n",
    "\n",
    "**Workflow** :\n",
    "1. Prédire l'intent du message\n",
    "2. Si intent = TRIP, extraire Departure et Destination avec NER\n",
    "3. Retourner le résultat formaté"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888604ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Charger les pipelines\n",
    "print(\"Chargement des modèles...\")\n",
    "\n",
    "# Pipeline Intent Classification\n",
    "intent_pipeline = pipeline(\n",
    "    'text-classification',\n",
    "    model='/content/drive/MyDrive/nlp_dataset/models/intent_classifier_best',\n",
    "    tokenizer='/content/drive/MyDrive/nlp_dataset/models/intent_classifier_best',\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Pipeline NER\n",
    "ner_pipeline = pipeline(\n",
    "    'token-classification',\n",
    "    model='/content/drive/MyDrive/nlp_dataset/models/ner_model_best',\n",
    "    tokenizer='/content/drive/MyDrive/nlp_dataset/models/ner_model_best',\n",
    "    aggregation_strategy='simple',\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "print(\"Pipelines chargés!\")\n",
    "\n",
    "def predict_travel_order(text):\n",
    "    \"\"\"\n",
    "    Pipeline complet : Intent + NER\n",
    "    \"\"\"\n",
    "    # 1. Prédire l'intent\n",
    "    intent_result = intent_pipeline(text)[0]\n",
    "    predicted_intent = intent_result['label']\n",
    "    confidence = intent_result['score']\n",
    "    \n",
    "    result = {\n",
    "        'text': text,\n",
    "        'intent': predicted_intent,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "    \n",
    "    # 2. Si TRIP, extraire les entités\n",
    "    if predicted_intent == 'TRIP':\n",
    "        ner_results = ner_pipeline(text)\n",
    "        \n",
    "        departure = None\n",
    "        destination = None\n",
    "        \n",
    "        for entity in ner_results:\n",
    "            entity_label = entity.get('entity_group', entity.get('entity'))\n",
    "            word = entity['word'].replace('▁', ' ').strip()\n",
    "            \n",
    "            if 'Departure' in entity_label and not departure:\n",
    "                departure = word\n",
    "            elif 'Destination' in entity_label and not destination:\n",
    "                destination = word\n",
    "        \n",
    "        result['departure'] = departure\n",
    "        result['destination'] = destination\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Tester sur quelques exemples\n",
    "print(\"\\nTests sur des exemples:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_examples = [\n",
    "    \"Je voudrais un billet de Cotonou à Porto-Novo demain\",\n",
    "    \"Bonjour, un aller-retour Paris Marseille s'il vous plaît\",\n",
    "    \"Merci pour votre email, je confirme ma présence\",\n",
    "    \"Hello, I need a ticket to London\",\n",
    "    \"zxqwerty azerty\"\n",
    "]\n",
    "\n",
    "for i, example in enumerate(test_examples, 1):\n",
    "    print(f\"\\n{i}. {example}\")\n",
    "    result = predict_travel_order(example)\n",
    "    print(f\"   Intent: {result['intent']} (confiance: {result['confidence']:.2%})\")\n",
    "    \n",
    "    if result['intent'] == 'TRIP':\n",
    "        dep = result.get('departure', 'N/A')\n",
    "        dest = result.get('destination', 'N/A')\n",
    "        print(f\"   Departure: {dep}\")\n",
    "        print(f\"   Destination: {dest}\")\n",
    "        print(f\"   → Sortie: {i},{dep},{dest}\")\n",
    "    else:\n",
    "        print(f\"   → Sortie: {i},{result['intent']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Pipeline d'inférence prêt à l'emploi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f327b0",
   "metadata": {},
   "source": [
    "## 11 - Traitement Batch d'un Fichier d'Input\n",
    "\n",
    "Lit un fichier d'input au format `sentenceID,sentence` et génère un fichier de sortie avec les prédictions.\n",
    "\n",
    "**Format d'entrée** : `1,Je voudrais un billet Paris Lyon`  \n",
    "**Format de sortie** :\n",
    "- Si TRIP : `1,Paris,Lyon`\n",
    "- Sinon : `1,NOT_TRIP` (ou UNKNOWN, NOT_FRENCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0408ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch_file(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Traite un fichier batch et génère les prédictions.\n",
    "    \n",
    "    Format entrée: sentenceID,sentence\n",
    "    Format sortie: sentenceID,departure,destination (si TRIP) ou sentenceID,intent (sinon)\n",
    "    \"\"\"\n",
    "    output_lines = []\n",
    "    \n",
    "    print(f\"Lecture de {input_file}...\")\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    print(f\"Traitement de {len(lines)} phrases...\")\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Parser la ligne\n",
    "            parts = line.split(',', 1)\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            \n",
    "            sentence_id, text = parts\n",
    "            \n",
    "            # Prédire\n",
    "            result = predict_travel_order(text)\n",
    "            \n",
    "            # Formater la sortie\n",
    "            if result['intent'] == 'TRIP':\n",
    "                departure = result.get('departure', 'UNKNOWN')\n",
    "                destination = result.get('destination', 'UNKNOWN')\n",
    "                \n",
    "                if departure and destination:\n",
    "                    output_line = f\"{sentence_id},{departure},{destination}\"\n",
    "                else:\n",
    "                    output_line = f\"{sentence_id},UNKNOWN\"\n",
    "            else:\n",
    "                output_line = f\"{sentence_id},{result['intent']}\"\n",
    "            \n",
    "            output_lines.append(output_line)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur sur la ligne: {line[:50]}... → {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Sauvegarder\n",
    "    print(f\"\\nSauvegarde de {len(output_lines)} résultats dans {output_file}...\")\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(output_lines))\n",
    "    \n",
    "    print(f\"Traitement terminé!\")\n",
    "    return output_file\n",
    "\n",
    "# Exemple d'utilisation\n",
    "input_file_path = '/content/drive/MyDrive/nlp_dataset/sample_nlp_input.txt'\n",
    "output_file_path = '/content/drive/MyDrive/nlp_dataset/predictions_output.txt'\n",
    "\n",
    "# Vérifier si le fichier existe\n",
    "if os.path.exists(input_file_path):\n",
    "    result_file = process_batch_file(input_file_path, output_file_path)\n",
    "    \n",
    "    print(f\"\\nAperçu des résultats:\")\n",
    "    print(\"=\"*70)\n",
    "    with open(result_file, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 10:  # Afficher les 10 premières lignes\n",
    "                break\n",
    "            print(line.strip())\n",
    "    \n",
    "    print(\"\\nFichier complet sauvegardé dans Google Drive!\")\n",
    "else:\n",
    "    print(f\"Fichier d'entrée non trouvé: {input_file_path}\")\n",
    "    print(\"   Uploadez votre fichier dans Google Drive ou modifiez le chemin.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34ff131",
   "metadata": {},
   "source": [
    "## Récapitulatif et Instructions d'Utilisation\n",
    "\n",
    "### Ce qui a été fait :\n",
    "1. **Fine-tuning Intent Classification** : Modèle CamemBERT entraîné pour classifier 4 types d'intent (TRIP, NOT_TRIP, UNKNOWN, NOT_FRENCH)\n",
    "2. **Fine-tuning NER** : Modèle CamemBERT entraîné pour extraire Departure et Destination\n",
    "3. **Pipeline d'inférence** : Système complet combinant les deux modèles\n",
    "4. **Traitement batch** : Fonction pour traiter des fichiers d'input en masse\n",
    "\n",
    "### Modèles sauvegardés dans Google Drive :\n",
    "- `/content/drive/MyDrive/nlp_dataset/models/intent_classifier_best/`\n",
    "- `/content/drive/MyDrive/nlp_dataset/models/ner_model_best/`\n",
    "\n",
    "### Pour réutiliser les modèles plus tard :\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Charger le pipeline Intent\n",
    "intent_pipe = pipeline(\n",
    "    'text-classification',\n",
    "    model='/content/drive/MyDrive/nlp_dataset/models/intent_classifier_best',\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Charger le pipeline NER\n",
    "ner_pipe = pipeline(\n",
    "    'token-classification',\n",
    "    model='/content/drive/MyDrive/nlp_dataset/models/ner_model_best',\n",
    "    aggregation_strategy='simple',\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Utiliser\n",
    "result = intent_pipe(\"Je veux aller de Cotonou à Porto-Novo\")\n",
    "entities = ner_pipe(\"Je veux aller de Cotonou à Porto-Novo\")\n",
    "```\n",
    "\n",
    "### Performances attendues :\n",
    "- **Intent Classification** : F1-Score > 0.95 (selon la qualité du dataset)\n",
    "- **NER** : F1-Score > 0.90 pour Departure et Destination\n",
    "\n",
    "### Optimisations possibles :\n",
    "1. Augmenter le nombre d'epochs pour améliorer les performances\n",
    "2. Ajuster les hyperparamètres (learning rate, batch size, weight decay)\n",
    "3. Essayer d'autres modèles français : FlauBERT, BARThez\n",
    "4. Ajouter de l'augmentation de données\n",
    "5. Utiliser des techniques d'ensemble (combiner plusieurs modèles)\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook complété avec succès !**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
