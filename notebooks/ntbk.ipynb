{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16d5033b",
   "metadata": {},
   "source": [
    "## Utilitaire : imports & paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f23c54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal, fftpack\n",
    "import math\n",
    "\n",
    "# paramètres communs\n",
    "SR = 16000          # sampling rate cible (Hz)\n",
    "FRAME_MS = 25       # taille trame en ms\n",
    "STRIDE_MS = 10      # pas entre trames en ms\n",
    "NFFT = 512          # FFT points\n",
    "PRE_EMPH = 0.97     # coefficient de pré-accentuation\n",
    "NFILT = 40          # nombre de filtres Mel\n",
    "NUM_CEPS = 13       # dimension MFCC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91646964",
   "metadata": {},
   "source": [
    "## Chargement et (re)échantillonnage\n",
    "\n",
    "### But :\n",
    "obtenir un tableau audio (float32) et la fréquence d’échantillonnage sr homogène. \n",
    "\n",
    "### Explication\n",
    "\n",
    "- Lire un fichier WAV/FLAC et convertir en mono float (−1.0 … 1.0).\n",
    "\n",
    "- Si l’échantillonnage n’est pas SR (ex. 16000 Hz), on resample pour homogénéiser les données d’entrée du pipeline.\n",
    "\n",
    "\n",
    "Entrée : fichier WAV stereo 44100 Hz (durée 3 s)\n",
    "→ audio non traité : dtype int16, shape (132300, 2).\n",
    "\n",
    "Sortie : (sr=16000, audio)\n",
    "→ dtype float32, shape (48000,), valeurs ≈ [-0.23, 0.12, 0.01, ...]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0293a5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple minimal : lire avec scipy (WAV mono) puis resampler si nécessaire\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal as sps\n",
    "\n",
    "def load_audio(path, target_sr=SR):\n",
    "    sr, audio = wavfile.read(path)                 # audio peut être int16 etc.\n",
    "    # normaliser en float32 entre -1 et 1\n",
    "    if audio.dtype == 'int16':\n",
    "        audio = audio.astype(np.float32) / 32768.0\n",
    "    elif audio.dtype == 'int32':\n",
    "        audio = audio.astype(np.float32) / 2147483648.0\n",
    "    elif audio.dtype == 'uint8':\n",
    "        audio = (audio.astype(np.float32) - 128) / 128.0\n",
    "    audio = audio.mean(axis=1) if audio.ndim == 2 else audio  # mono\n",
    "    if sr != target_sr:\n",
    "        # resampling\n",
    "        number_of_samples = round(len(audio) * float(target_sr) / sr)\n",
    "        audio = sps.resample(audio, number_of_samples)\n",
    "        sr = target_sr\n",
    "    return sr, audio\n",
    "\n",
    "sr, audio = load_audio(\"../data/atoutalheure.wav\", target_sr=16000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0459f470",
   "metadata": {},
   "source": [
    "## Pré-accentuation (pre-emphasis)\n",
    "\n",
    "### But : \n",
    "renforcer les hautes fréquences pour compenser la baisse énergétique et améliorer la détection de certains traits (consonnes/fricatives).\n",
    "\n",
    "### Explication\n",
    "\n",
    "- Filtre simple : y[n] = x[n] - a * x[n-1] (avec a ≈ 0.95–0.98).\n",
    "- Effet : augmente la pente du spectre (atténue composantes basses, met en valeur hautes).\n",
    "\n",
    "\n",
    "Entrée : audio[:8] = [0.0, 0.1, 0.15, 0.12, 0.08, -0.02, -0.04, 0.0]\n",
    "\n",
    "Sortie (a=0.97) : [0.0, 0.003, 0.022, -0.0189, -0.0556, -0.0966, -0.0408, 0.0388]\n",
    "(les valeurs illustratives montrent l’augmentation relative des variations rapides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ce4f271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_emphasis(signal_in, pre_emph=PRE_EMPH):\n",
    "    return np.append(signal_in[0], signal_in[1:] - pre_emph * signal_in[:-1])\n",
    "\n",
    "# Usage\n",
    "audio_preemph = pre_emphasis(audio, pre_emph=0.97)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095939bf",
   "metadata": {},
   "source": [
    "## Tramage (framing) + fenêtrage (Hamming)\n",
    "\n",
    "### But : \n",
    "découper le signal en petites trames quasi-stationnaires (p.ex. 25 ms) avec chevauchement (p.ex. stride 10 ms) puis appliquer une fenêtre Hamming.\n",
    "\n",
    "### Explication\n",
    "\n",
    "- On suppose la stationnarité sur ~20–30 ms.\n",
    "\n",
    "- On crée un tableau frames shape (num_frames, frame_len) et on multiplie chaque trame par une fenêtre hamming(frame_len).\n",
    "\n",
    "\n",
    "Entrée : audio_preemph length 48000 (3 s @ 16kHz)\n",
    "\n",
    "Sortie : frames_win shape (num_frames= (48000-400)/160 + 1 ≈ 296, frame_len=400) (25 ms → 400 samples)\n",
    "\n",
    "frames_win[0] : vecteur de 400 floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6fdc82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enframe(signal_in, sr=SR, frame_ms=FRAME_MS, stride_ms=STRIDE_MS):\n",
    "    frame_len = int(sr * frame_ms / 1000)\n",
    "    frame_step = int(sr * stride_ms / 1000)\n",
    "    signal_length = len(signal_in)\n",
    "    num_frames = 1 + int(np.floor((signal_length - frame_len) / frame_step))\n",
    "    pad_len = num_frames * frame_step + frame_len\n",
    "    pad_signal = np.append(signal_in, np.zeros(pad_len - signal_length))\n",
    "    # stride trick pour efficacité (ici code simple)\n",
    "    frames = np.stack([pad_signal[i*frame_step : i*frame_step + frame_len] for i in range(num_frames)])\n",
    "    win = np.hamming(frame_len)\n",
    "    return frames * win, frame_len, frame_step\n",
    "\n",
    "# Usage\n",
    "frames_win, frame_len, frame_step = enframe(audio_preemph, sr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c22b216",
   "metadata": {},
   "source": [
    "## Spectre de puissance (power spectrum)\n",
    "\n",
    "### But : \n",
    "pour chaque trame, calculer la FFT (magnitude ou puissance) — base pour spectrogramme / filtration Mel.\n",
    "\n",
    "### Explication\n",
    "\n",
    "- On calcule X[k] = FFT(frame, NFFT) puis |X[k]| ou puissance |X[k]|^2 / NFFT.\n",
    "\n",
    "- Pour gain de compacité on conserve uniquement les bins réelles : rfft.\n",
    "\n",
    "\n",
    "Entrée : frames_win shape (296, 400)\n",
    "\n",
    "Sortie : pow_frames shape (296, 257) (pour NFFT=512)\n",
    "\n",
    "- pow_frames[0, :10] : [1.2e-5, 3.3e-6, 2.7e-6, ...] (valeurs d’énergie par bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21869547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_spectrum(frames_win, NFFT=NFFT):\n",
    "    # frames_win: (num_frames, frame_len)\n",
    "    mag = np.abs(np.fft.rfft(frames_win, n=NFFT, axis=1))\n",
    "    pow_spec = (1.0 / NFFT) * (mag ** 2)\n",
    "    return pow_spec  # shape (num_frames, NFFT/2+1)\n",
    "\n",
    "# Usage\n",
    "pow_frames = power_spectrum(frames_win)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f480a1e7",
   "metadata": {},
   "source": [
    "## Banque de filtres Mel → Log-Mel Spectrum\n",
    "\n",
    "### But : \n",
    "regrouper l’énergie spectrale selon l’échelle Mel (perceptuelle) puis prendre le log — vecteur robuste et compact.\n",
    "\n",
    "### Explication\n",
    "\n",
    "- Convertir fréquences linéaires → mel (non-linéaire), placer NFILT triangles entre 0 et Nyquist.\n",
    "\n",
    "- Appliquer chaque filtre sur pow_frames pour obtenir filter_banks shape (num_frames, NFILT).\n",
    "\n",
    "- Prendre log (ou 20*log10) pour correspondre à la perception loudness.\n",
    "\n",
    "\n",
    "Entrée : pow_frames shape (296, 257)\n",
    "\n",
    "Sortie : log_mel shape (296, 40)\n",
    "\n",
    "- log_mel[0, :5] : [-45.2, -50.1, -42.9, -39.8, -55.0] (dB approximatifs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f915aa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hz_to_mel(hz): return 2595.0 * np.log10(1 + hz / 700.0)\n",
    "def mel_to_hz(mel): return 700.0 * (10**(mel / 2595.0) - 1)\n",
    "\n",
    "def mel_filterbanks(pow_frames, sr=SR, nfilt=NFILT, NFFT=NFFT):\n",
    "    low_freq = 0\n",
    "    high_freq = sr / 2\n",
    "    low_mel = hz_to_mel(low_freq)\n",
    "    high_mel = hz_to_mel(high_freq)\n",
    "    mel_points = np.linspace(low_mel, high_mel, nfilt + 2)\n",
    "    hz_points = mel_to_hz(mel_points)\n",
    "    bin_points = np.floor((NFFT + 1) * hz_points / sr).astype(int)\n",
    "    fbank = np.zeros((nfilt, int(np.floor(NFFT/2 + 1))))\n",
    "    for m in range(1, nfilt+1):\n",
    "        f_m_minus = bin_points[m-1]; f_m = bin_points[m]; f_m_plus = bin_points[m+1]\n",
    "        for k in range(f_m_minus, f_m):\n",
    "            fbank[m-1, k] = (k - f_m_minus) / (f_m - f_m_minus + 1e-8)\n",
    "        for k in range(f_m, f_m_plus):\n",
    "            fbank[m-1, k] = (f_m_plus - k) / (f_m_plus - f_m + 1e-8)\n",
    "    filter_banks = np.dot(pow_frames, fbank.T)\n",
    "    # numerical stability\n",
    "    filter_banks = np.where(filter_banks == 0, np.finfo(float).eps, filter_banks)\n",
    "    log_fbanks = 20 * np.log10(filter_banks)\n",
    "    return log_fbanks  # shape (num_frames, nfilt)\n",
    "\n",
    "# Usage\n",
    "log_mel = mel_filterbanks(pow_frames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7af8d00",
   "metadata": {},
   "source": [
    "## MFCC (Mel Frequency Cepstral Coefficients)\n",
    "\n",
    "### But : \n",
    "obtenir des coefficients compacts et peu corrélés (par DCT) à partir des log-Mel.\n",
    "\n",
    "### Explication\n",
    "\n",
    "- Appliquer la DCT (type II) sur chaque vecteur log-Mel et conserver NUM_CEPS premières composantes.\n",
    "\n",
    "- On applique souvent un cepstral lifter (poids) et on peut ajouter les deltas/delta-deltas (dérivées temporelles).\n",
    "\n",
    "\n",
    "Entrée : log_mel shape (296, 40)\n",
    "\n",
    "Sortie : mfcc shape (296, 13)\n",
    "\n",
    "- mfcc[0] : [ -2.12, 1.34, -0.45, 0.12, ... ] (valeurs unitless, données d’apprentissage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13435e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfcc_from_log_mel(log_mel, num_ceps=NUM_CEPS, lifter=22):\n",
    "    # log_mel: (num_frames, nfilt)\n",
    "    ceps = fftpack.dct(log_mel, type=2, axis=1, norm='ortho')[:, :num_ceps]\n",
    "    # liftering\n",
    "    n = np.arange(num_ceps)\n",
    "    lift = 1 + (lifter/2.) * np.sin(np.pi * n / lifter)\n",
    "    ceps *= lift\n",
    "    return ceps  # shape (num_frames, num_ceps)\n",
    "\n",
    "# optionally compute deltas:\n",
    "def deltas(feat, N=2):\n",
    "    rows, cols = feat.shape\n",
    "    denom = 2 * sum([i*i for i in range(1, N+1)])\n",
    "    delta_feat = np.empty_like(feat)\n",
    "    padded = np.pad(feat, ((N,N),(0,0)), mode='edge')\n",
    "    for t in range(rows):\n",
    "        delta_feat[t] = sum([n * (padded[t+N+n] - padded[t+N-n]) for n in range(1, N+1)]) / denom\n",
    "    return delta_feat\n",
    "\n",
    "# Usage\n",
    "mfcc = mfcc_from_log_mel(log_mel)\n",
    "mfcc_delta = deltas(mfcc)\n",
    "mfcc_ddelta = deltas(mfcc_delta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e1d92c",
   "metadata": {},
   "source": [
    "## Pitch (F0) & RMS (énergie)\n",
    "\n",
    "### But : \n",
    "extraire des features prosodiques — fondamental pour la prosodie / TTS / diarisation / certaines tâches ASR.\n",
    "\n",
    "### Explication\n",
    "\n",
    "- Méthodes courantes : autocorr + peak picking (bon pour voix régulière) ou méthodes plus robustes (YIN, pYIN, librosa.pyin).\n",
    "\n",
    "- RMS : énergie par trame = sqrt(mean(frame^2)).\n",
    "\n",
    "\n",
    "Entrée : la trame frames_win[50] (voisée)\n",
    "\n",
    "Sortie : f0 ≈ 120.0 Hz (valeur si la frame contient une voix fondée sur F0=120Hz)\n",
    "\n",
    "RMS : 0.03 (énergie moyenne, unité = amplitude audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f41bd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorr_pitch(frame, sr=SR, fmin=50, fmax=400):\n",
    "    # renvoyer 0 si non voiced / pas d'estimation\n",
    "    corr = np.correlate(frame, frame, mode='full')[len(frame)-1:]\n",
    "    min_lag = int(sr / fmax)\n",
    "    max_lag = int(sr / fmin)\n",
    "    if max_lag >= len(corr): return 0.0\n",
    "    peak_lag = np.argmax(corr[min_lag:max_lag]) + min_lag\n",
    "    peak_val = corr[peak_lag]\n",
    "    if peak_val < 1e-6: return 0.0\n",
    "    return sr / peak_lag\n",
    "\n",
    "def pitch_track(frames_win, sr=SR):\n",
    "    return np.array([autocorr_pitch(frames_win[i], sr) for i in range(frames_win.shape[0])])\n",
    "\n",
    "def rms_energy(frames_win):\n",
    "    return np.sqrt(np.mean(frames_win**2, axis=1))\n",
    "\n",
    "# Usage\n",
    "f0s = pitch_track(frames_win)\n",
    "rms = rms_energy(frames_win)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8025697",
   "metadata": {},
   "source": [
    "## Normalisation (CMVN : cepstral mean & variance normalization)\n",
    "\n",
    "### But : \n",
    "rendre les features invariantes aux différences d’enregistrement / gain. Très utile avant d’alimenter un modèle.\n",
    "\n",
    "### Explication\n",
    "\n",
    "- On centre (soustrait la moyenne) et on divise par l’écart-type par dimension : x' = (x - mean)/std.\n",
    "\n",
    "- Alternatives : normalisation sur fenêtres (sliding CMVN).\n",
    "\n",
    "\n",
    "Entrée : mfcc shape (296, 13)\n",
    "\n",
    "Sortie : mfcc_norm shape (296, 13) où chaque colonne a mean≈0 et std≈1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "528e1954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmvn(features, eps=1e-10):\n",
    "    mu = np.mean(features, axis=0)\n",
    "    sigma = np.std(features, axis=0)\n",
    "    return (features - mu) / (sigma + eps)\n",
    "\n",
    "# Usage\n",
    "mfcc_norm = cmvn(mfcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "baed6023",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'acoustic_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# inputs: mfcc_norm (T x D)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# acoustic_model: model -> logits (T x V) (V = taille vocab. de tokens)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# lm: language model scoring function (optional)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m logits = \u001b[43macoustic_model\u001b[49m(mfcc_norm)  \u001b[38;5;66;03m# shape (T, V)\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# softmax pour obtenir probabilités\u001b[39;00m\n\u001b[32m      7\u001b[39m probs = softmax(logits, axis=\u001b[32m1\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'acoustic_model' is not defined"
     ]
    }
   ],
   "source": [
    "# inputs: mfcc_norm (T x D)\n",
    "# acoustic_model: model -> logits (T x V) (V = taille vocab. de tokens)\n",
    "# lm: language model scoring function (optional)\n",
    "\n",
    "logits = acoustic_model(mfcc_norm)  # shape (T, V)\n",
    "# softmax pour obtenir probabilités\n",
    "probs = softmax(logits, axis=1)\n",
    "\n",
    "# Beam search (très schématique)\n",
    "def ctc_beam_search(probs, beam_width=10, lm=None):\n",
    "    beams = [(\"\", 1.0)]  # (prefix, score)\n",
    "    for t in range(probs.shape[0]):\n",
    "        new_beams = {}\n",
    "        for prefix, score in beams:\n",
    "            for token in topk_indices(probs[t], k=beam_width):\n",
    "                p_token = probs[t, token]\n",
    "                new_prefix = extend(prefix, token)  # gère tokens blanks/merge\n",
    "                new_score = score * p_token\n",
    "                if lm: new_score *= lm.score_partial(new_prefix)\n",
    "                # keep best per prefix\n",
    "                if new_prefix not in new_beams or new_beams[new_prefix] < new_score:\n",
    "                    new_beams[new_prefix] = new_score\n",
    "        # keep top-K beams\n",
    "        beams = sorted(new_beams.items(), key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "    return beams[0][0]  # best hypothesis after T frames\n",
    "\n",
    "# Note: en pratique on utilise des librairies (ctcdecode, warp-ctc, fairseq, huggingface) solide.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
