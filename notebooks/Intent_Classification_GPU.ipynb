{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb0ab73f",
   "metadata": {},
   "source": [
    "# Travel Order Resolver â€” Notebook\n",
    "This notebook implements an intent classifier + NER (Departure/Destination) for the **Travel Order Resolver** project.\n",
    "It uses **CamemBERT** (transfer learning) via Hugging Face Transformers.\n",
    "\n",
    "**What is included**\n",
    "- Installation of dependencies\n",
    "- Loading dataset (from Google Drive or upload)\n",
    "- Preprocessing and tokenization\n",
    "- Fine-tuning (intent classification) and token-classification (NER)\n",
    "- Inference pipeline that reads `sentenceID,sentence` and writes outputs in the required format\n",
    "\n",
    "**Important notes**\n",
    "- The GPU notebook is optimized to run on Google Colab with a GPU runtime.\n",
    "- The CPU notebook is lighter and intended for local execution without a GPU (slower).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## GPU notebook (Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b67f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (GPU / Colab)\n",
    "!pip install -q transformers datasets evaluate seqeval accelerate tokenizers sacrebleu\n",
    "print(\"Dependencies installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2596a696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (optional) to load dataset from Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# Example path in Drive: /content/drive/MyDrive/nlp_miniprojects/train_set.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2695f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset (adjust paths as needed)\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Default paths (if you uploaded files to Colab /content/)\n",
    "train_csv = \"/content/train_set.csv\"\n",
    "test_csv = \"/content/test_set.csv\"\n",
    "\n",
    "# If using Drive, change to the drive path (example shown in previous cell)\n",
    "if not os.path.exists(train_csv) or not os.path.exists(test_csv):\n",
    "    print(\"Train or test CSV not found in /content/. If using Drive, set the paths to your Drive folder.\")\n",
    "else:\n",
    "    train_df = pd.read_csv(train_csv, encoding=\"utf-8\")\n",
    "    test_df = pd.read_csv(test_csv, encoding=\"utf-8\")\n",
    "    print(\"Train shape:\", train_df.shape, \"Test shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f319ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inspect and parse 'entities' JSON field (if present)\n",
    "import json\n",
    "def parse_entities_field(row):\n",
    "    try:\n",
    "        ents = json.loads(row['entities'])\n",
    "    except Exception:\n",
    "        ents = []\n",
    "    valid = []\n",
    "    for ent in ents:\n",
    "        if 'start' in ent and 'end' in ent and 0 <= ent['start'] < ent['end'] <= len(row['text']):\n",
    "            valid.append(ent)\n",
    "    return valid\n",
    "\n",
    "train_df['parsed_entities'] = train_df.apply(parse_entities_field, axis=1)\n",
    "test_df['parsed_entities'] = test_df.apply(parse_entities_field, axis=1)\n",
    "print('Parsed entities (example):')\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc6d210",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare Hugging Face datasets for intent classification\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_df['intent'].unique())\n",
    "num_labels = len(label_encoder.classes_)\n",
    "print(\"Intent classes:\", list(label_encoder.classes_))\n",
    "\n",
    "hf_train = Dataset.from_pandas(train_df[['text','intent']].rename(columns={'intent':'label'}))\n",
    "hf_test  = Dataset.from_pandas(test_df[['text','intent']].rename(columns={'intent':'label'}))\n",
    "\n",
    "def encode_label(example):\n",
    "    example['label'] = int(label_encoder.transform([example['label']])[0])\n",
    "    return example\n",
    "\n",
    "hf_train = hf_train.map(encode_label)\n",
    "hf_test = hf_test.map(encode_label)\n",
    "\n",
    "model_name = \"camembert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_classification(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "hf_train = hf_train.map(tokenize_classification, batched=True, remove_columns=['text'])\n",
    "hf_test = hf_test.map(tokenize_classification, batched=True, remove_columns=['text'])\n",
    "\n",
    "hf_train = hf_train.rename_column(\"label\", \"labels\")\n",
    "hf_test = hf_test.rename_column(\"label\", \"labels\")\n",
    "hf_train.set_format(type=\"torch\")\n",
    "hf_test.set_format(type=\"torch\")\n",
    "\n",
    "print(hf_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351e6e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fine-tune CamemBERT for Intent Classification (GPU-optimized args)\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import evaluate, numpy as np\n",
    "\n",
    "model_cls = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics_intent(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    f1 = metric.compute(predictions=preds, references=labels, average=\"macro\")['f1']\n",
    "    acc = (preds == labels).mean()\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./camembert-intent\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_cls,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_test,\n",
    "    compute_metrics=compute_metrics_intent,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./camembert-intent-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad23b172",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare NER dataset (token-classification) - convert spans to BIO using fast tokenizer offsets\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer_fast = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "ner_labels = ['O', 'B-Departure', 'I-Departure', 'B-Destination', 'I-Destination']\n",
    "label2id = {l:i for i,l in enumerate(ner_labels)}\n",
    "id2label = {i:l for l,i in label2id.items()}\n",
    "\n",
    "def spans_to_bio(text, spans):\n",
    "    encoding = tokenizer_fast(text, return_offsets_mapping=True, truncation=True, max_length=128)\n",
    "    offsets = encoding['offset_mapping']\n",
    "    labels = ['O'] * len(offsets)\n",
    "    for ent in spans:\n",
    "        st, ed = ent['start'], ent['end']\n",
    "        token_indices = []\n",
    "        for i,(a,b) in enumerate(offsets):\n",
    "            if a==b==0:\n",
    "                continue\n",
    "            if not (b <= st or a >= ed):\n",
    "                token_indices.append(i)\n",
    "        if not token_indices:\n",
    "            continue\n",
    "        labels[token_indices[0]] = 'B-' + ent['label']\n",
    "        for idx in token_indices[1:]:\n",
    "            labels[idx] = 'I-' + ent['label']\n",
    "    label_ids = [label2id.get(l,0) for l in labels]\n",
    "    return encoding, label_ids\n",
    "\n",
    "# Build HF datasets (train/test)\n",
    "from datasets import Dataset\n",
    "ner_rows = []\n",
    "for _, r in train_df.iterrows():\n",
    "    text = r['text']\n",
    "    spans = r['parsed_entities'] if r['intent']=='TRIP' else []\n",
    "    _, label_ids = spans_to_bio(text, spans)\n",
    "    ner_rows.append({'text': text, 'labels': label_ids})\n",
    "\n",
    "ner_train_ds = Dataset.from_list(ner_rows)\n",
    "\n",
    "ner_rows_test = []\n",
    "for _, r in test_df.iterrows():\n",
    "    text = r['text']\n",
    "    spans = r['parsed_entities'] if r['intent']=='TRIP' else []\n",
    "    _, label_ids = spans_to_bio(text, spans)\n",
    "    ner_rows_test.append({'text': text, 'labels': label_ids})\n",
    "\n",
    "ner_test_ds = Dataset.from_list(ner_rows_test)\n",
    "print('NER datasets prepared (counts):', len(ner_train_ds), len(ner_test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caefa652",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fine-tune CamemBERT for NER (GPU-optimized)\n",
    "from transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    "import numpy as np, seqeval.metrics as seq_metrics\n",
    "\n",
    "model_ner = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(ner_labels), id2label=id2label, label2id=label2id)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer_fast)\n",
    "\n",
    "# Tokenize texts and align labels (we'll create input_ids and attention_mask)\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer_fast(examples['text'], truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
    "    # We already have labels aligned to token offsets in 'labels'; but to keep notebook concise,\n",
    "    # we keep labels as precomputed per token position (expected length <= max_length)\n",
    "    return {'input_ids': tokenized_inputs['input_ids'].tolist(), 'attention_mask': tokenized_inputs['attention_mask'].tolist(), 'labels': examples['labels']}\n",
    "\n",
    "ner_train_tok = ner_train_ds.map(lambda x: tokenize_and_align_labels(x), batched=True)\n",
    "ner_test_tok = ner_test_ds.map(lambda x: tokenize_and_align_labels(x), batched=True)\n",
    "\n",
    "ner_train_tok.set_format(type='torch', columns=['input_ids','attention_mask','labels'])\n",
    "ner_test_tok.set_format(type='torch', columns=['input_ids','attention_mask','labels'])\n",
    "\n",
    "def align_preds(predictions, label_ids):\n",
    "    preds = np.argmax(predictions, axis=-1)\n",
    "    preds_list = [[id2label[p] for p in pred] for pred in preds]\n",
    "    labels_list = [[id2label[l] for l in lab] for lab in label_ids]\n",
    "    return preds_list, labels_list\n",
    "\n",
    "def compute_metrics_ner(p):\n",
    "    preds, labels = p\n",
    "    preds_list, labels_list = align_preds(preds, labels)\n",
    "    return {\n",
    "        'precision': seq_metrics.precision_score(labels_list, preds_list),\n",
    "        'recall': seq_metrics.recall_score(labels_list, preds_list),\n",
    "        'f1': seq_metrics.f1_score(labels_list, preds_list)\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./camembert-ner',\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy='epoch',\n",
    "    num_train_epochs=3,\n",
    "    save_strategy='epoch',\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "trainer_ner = Trainer(\n",
    "    model=model_ner,\n",
    "    args=training_args,\n",
    "    train_dataset=ner_train_tok,\n",
    "    eval_dataset=ner_test_tok,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer_fast,\n",
    "    compute_metrics=compute_metrics_ner\n",
    ")\n",
    "\n",
    "trainer_ner.train()\n",
    "trainer_ner.save_model('./camembert-ner-best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6698512",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inference pipeline (load saved models and run on new lines)\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "# Load intent model\n",
    "intent_tokenizer = AutoTokenizer.from_pretrained('./camembert-intent-best')\n",
    "intent_model = AutoModelForSequenceClassification.from_pretrained('./camembert-intent-best')\n",
    "\n",
    "# Load ner model\n",
    "ner_tokenizer = AutoTokenizer.from_pretrained('./camembert-ner-best', use_fast=True)\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained('./camembert-ner-best')\n",
    "\n",
    "intent_pipe = pipeline('text-classification', model=intent_model, tokenizer=intent_tokenizer)\n",
    "ner_pipe = pipeline('token-classification', model=ner_model, tokenizer=ner_tokenizer, aggregation_strategy='simple')\n",
    "\n",
    "# Example inference function\n",
    "def predict_line(sentenceID, sentence):\n",
    "    # intent\n",
    "    inputs = intent_tokenizer(sentence, return_tensors='pt', truncation=True, max_length=128)\n",
    "    logits = intent_model(**inputs).logits.detach().cpu().numpy()[0]\n",
    "    pred_id = int(logits.argmax())\n",
    "    print('Pred intent id:', pred_id)\n",
    "    # NER\n",
    "    ner_res = ner_pipe(sentence)\n",
    "    print('NER result:', ner_res)\n",
    "    return None\n",
    "\n",
    "# Example usage\n",
    "print(predict_line('1', 'Je voudrais un billet Toulouse Paris.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8d6c39",
   "metadata": {},
   "source": [
    "\n",
    "# Final notes\n",
    "- The GPU notebook is for Colab with a GPU runtime (recommended). Use Runtime -> Change runtime type -> GPU.\n",
    "- The CPU notebook is slower; training on a CPU may take a long time.\n",
    "- Save your models in the drive if you want to keep them across sessions (example path: /content/drive/MyDrive/nlp_miniprojects/).\n",
    "- Save the tokenizer and label mappings alongside the model for correct inference later.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
