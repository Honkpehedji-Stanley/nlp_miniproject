{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee25b6d1",
   "metadata": {},
   "source": [
    "# Travel Intent Classification & NER - Google Colab GPU\n",
    "\n",
    "Ce notebook Colab (optimis√© pour **GPU**) utilise **Transformers** pour du **Fine-Tuning** sur :\n",
    "1. **Intent Classification** : Classification en 4 classes (TRIP, NOT_TRIP, UNKNOWN, NOT_FRENCH)\n",
    "2. **Named Entity Recognition (NER)** : Extraction de Departure et Destination\n",
    "\n",
    "**Dataset** : Charg√© depuis Google Drive (`/content/drive/MyDrive/nlp_dataset/`)\n",
    "\n",
    "**Mod√®le** : CamemBERT (mod√®le fran√ßais pr√©-entra√Æn√©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c497b897",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Travel Order Resolver - Google Colab GPU + Transformers Fine-Tuning')\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9c9b9f",
   "metadata": {},
   "source": [
    "## 1 - V√©rification du GPU et Installation des d√©pendances\n",
    "\n",
    "V√©rifie la disponibilit√© du GPU et installe les librairies n√©cessaires :\n",
    "- `transformers` : Pour CamemBERT et le fine-tuning\n",
    "- `datasets` : Pour g√©rer les datasets\n",
    "- `evaluate` : Pour les m√©triques d'√©valuation\n",
    "- `seqeval` : Pour les m√©triques NER\n",
    "- `accelerate` : Pour l'optimisation GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fffb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier la disponibilit√© du GPU\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"Pas de GPU d√©tect√©. Allez dans Runtime > Change runtime type > GPU\")\n",
    "\n",
    "# Installation des packages\n",
    "!pip install -q transformers datasets evaluate seqeval accelerate\n",
    "print(\"Installation termin√©e!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb79d1a6",
   "metadata": {},
   "source": [
    "## 2 - Montage de Google Drive et Chargement des Datasets\n",
    "\n",
    "Monte Google Drive pour acc√©der aux datasets depuis `/content/drive/MyDrive/nlp_dataset/`\n",
    "\n",
    "**Structure attendue dans Drive** :\n",
    "```\n",
    "MyDrive/\n",
    "  nlp_dataset/\n",
    "    train_set.csv\n",
    "    test_set.csv\n",
    "    cities_fr.txt (optionnel)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87de4972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monter Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Chemins vers les datasets dans Google Drive\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "DATASET_DIR = '/content/drive/MyDrive/nlp_dataset'\n",
    "train_csv = os.path.join(DATASET_DIR, 'train_set.csv')\n",
    "test_csv = os.path.join(DATASET_DIR, 'test_set.csv')\n",
    "cities_file = os.path.join(DATASET_DIR, 'cities_fr.txt')\n",
    "\n",
    "# V√©rifier l'existence des fichiers\n",
    "if not os.path.exists(train_csv) or not os.path.exists(test_csv):\n",
    "    print(\"ERREUR: Fichiers introuvables!\")\n",
    "    print(f\"   Assurez-vous que les fichiers sont dans: {DATASET_DIR}\")\n",
    "    print(\"   - train_set.csv\")\n",
    "    print(\"   - test_set.csv\")\n",
    "else:\n",
    "    print(\" Fichiers trouv√©s!\")\n",
    "    train_df = pd.read_csv(train_csv, encoding='utf-8')\n",
    "    test_df = pd.read_csv(test_csv, encoding='utf-8')\n",
    "    print(f\"\\n Train shape: {train_df.shape}\")\n",
    "    print(f\" Test shape: {test_df.shape}\")\n",
    "    print(f\"\\n Distribution des classes (Train):\")\n",
    "    print(train_df['intent'].value_counts())\n",
    "    print(f\"\\n Aper√ßu du dataset:\")\n",
    "    display(train_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fd1ddd",
   "metadata": {},
   "source": [
    "## 3 - Pr√©traitement : Parser les Entities JSON\n",
    "\n",
    "Parse la colonne `entities` (format JSON) pour extraire les annotations Departure/Destination.\n",
    "\n",
    "**Format des entities** :\n",
    "```json\n",
    "[{\"start\": 10, \"end\": 15, \"label\": \"Departure\"}, {\"start\": 20, \"end\": 25, \"label\": \"Destination\"}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659b0858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def parse_entities_field(row):\n",
    "    \"\"\"Parse la colonne entities (JSON) et valide les annotations.\"\"\"\n",
    "    try:\n",
    "        ents = json.loads(row['entities'])\n",
    "    except Exception:\n",
    "        ents = []\n",
    "    \n",
    "    valid = []\n",
    "    txt = row.get('text', '')\n",
    "    for ent in ents:\n",
    "        if 'start' in ent and 'end' in ent and 'label' in ent:\n",
    "            if 0 <= ent['start'] < ent['end'] <= len(txt):\n",
    "                valid.append(ent)\n",
    "    return valid\n",
    "\n",
    "# Appliquer le parsing\n",
    "train_df['parsed_entities'] = train_df.apply(parse_entities_field, axis=1)\n",
    "test_df['parsed_entities'] = test_df.apply(parse_entities_field, axis=1)\n",
    "\n",
    "print(\"Entities pars√©es avec succ√®s!\")\n",
    "print(f\"\\nExemple avec entities:\")\n",
    "trip_examples = train_df[train_df['intent'] == 'TRIP'].head(2)\n",
    "for idx, row in trip_examples.iterrows():\n",
    "    print(f\"\\nTexte: {row['text']}\")\n",
    "    print(f\"Intent: {row['intent']}\")\n",
    "    print(f\"Entities: {row['parsed_entities']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd48f7c",
   "metadata": {},
   "source": [
    "## 4 - Pr√©paration des Datasets HuggingFace\n",
    "\n",
    "Conversion des DataFrames Pandas en Datasets HuggingFace pour l'entra√Ænement avec Transformers.\n",
    "\n",
    "Les datasets seront pr√©par√©s pour :\n",
    "1. **Intent Classification** : Text ‚Üí Label (TRIP, NOT_TRIP, UNKNOWN, NOT_FRENCH)\n",
    "2. **NER** : Text ‚Üí Tokens avec labels BIO (B-Departure, I-Departure, B-Destination, I-Destination, O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb4df48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# === DATASET POUR INTENT CLASSIFICATION ===\n",
    "\n",
    "# Encoder les labels d'intent\n",
    "label_encoder = LabelEncoder()\n",
    "all_intents = list(train_df['intent'].unique()) + list(test_df['intent'].unique())\n",
    "label_encoder.fit(list(set(all_intents)))\n",
    "\n",
    "print(\"Classes d'intent:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"   {i}: {label}\")\n",
    "\n",
    "# Cr√©er les datasets HuggingFace\n",
    "train_df['label'] = label_encoder.transform(train_df['intent'])\n",
    "test_df['label'] = label_encoder.transform(test_df['intent'])\n",
    "\n",
    "intent_train_dataset = Dataset.from_pandas(train_df[['text', 'label']])\n",
    "intent_test_dataset = Dataset.from_pandas(test_df[['text', 'label']])\n",
    "\n",
    "print(f\"\\nIntent datasets cr√©√©s:\")\n",
    "print(f\"   Train: {len(intent_train_dataset)} exemples\")\n",
    "print(f\"   Test: {len(intent_test_dataset)} exemples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8129008d",
   "metadata": {},
   "source": [
    "## 5 - Fine-Tuning Intent Classification avec CamemBERT\n",
    "\n",
    "Entra√Ænement d'un mod√®le de classification d'intent bas√© sur **CamemBERT** (mod√®le fran√ßais).\n",
    "\n",
    "**Param√®tres** :\n",
    "- Mod√®le : `camembert-base`\n",
    "- Nombre d'epochs : 3\n",
    "- Batch size : 16 (train) / 32 (eval)\n",
    "- Learning rate : 2e-5\n",
    "- Optimisation : AdamW avec weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efd4bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©sactiver WandB (tracking des exp√©riences)\n",
    "import os\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "print(\"‚úÖ WandB d√©sactiv√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7586d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Charger le tokenizer CamemBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained('camembert-base')\n",
    "\n",
    "# Fonction de tokenization\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# Tokenizer les datasets\n",
    "tokenized_train = intent_train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = intent_test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Charger le mod√®le CamemBERT pour classification\n",
    "num_labels = len(label_encoder.classes_)\n",
    "intent_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'camembert-base',\n",
    "    num_labels=num_labels,\n",
    "    id2label={i: label for i, label in enumerate(label_encoder.classes_)},\n",
    "    label2id={label: i for i, label in enumerate(label_encoder.classes_)}\n",
    ")\n",
    "\n",
    "# D√©finir les m√©triques\n",
    "accuracy_metric = evaluate.load('accuracy')\n",
    "f1_metric = evaluate.load('f1')\n",
    "\n",
    "def compute_intent_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='macro')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy['accuracy'],\n",
    "        'f1_macro': f1['f1']\n",
    "    }\n",
    "\n",
    "# Arguments d'entra√Ænement (optimis√©s pour GPU)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/content/drive/MyDrive/nlp_dataset/models/intent_classifier',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1_macro',\n",
    "    logging_steps=50,\n",
    "    warmup_steps=100,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    push_to_hub=False,\n",
    "    report_to='none',  # D√©sactiver WandB et autres trackers\n",
    ")\n",
    "\n",
    "# Cr√©er le Trainer\n",
    "intent_trainer = Trainer(\n",
    "    model=intent_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_intent_metrics\n",
    ")\n",
    "\n",
    "print(\"üöÄ D√©but du fine-tuning Intent Classification...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Entra√Æner le mod√®le\n",
    "intent_trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Fine-tuning Intent Classification termin√©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd00c5d",
   "metadata": {},
   "source": [
    "## 6 - √âvaluation du Mod√®le Intent Classification\n",
    "\n",
    "√âvalue le mod√®le entra√Æn√© sur le test set et affiche les m√©triques d√©taill√©es :\n",
    "- **Accuracy** : Pr√©cision globale\n",
    "- **F1-Score (Macro)** : F1 moyen sur toutes les classes\n",
    "- **Matrice de confusion** : Visualisation des erreurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfcc747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# √âvaluer sur le test set\n",
    "eval_results = intent_trainer.evaluate()\n",
    "\n",
    "print(\"R√©sultats sur le Test Set:\")\n",
    "print(\"=\"*70)\n",
    "for key, value in eval_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"   {key}: {value:.4f}\")\n",
    "\n",
    "# Pr√©dictions d√©taill√©es\n",
    "predictions = intent_trainer.predict(tokenized_test)\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=-1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(\n",
    "    true_labels,\n",
    "    predicted_labels,\n",
    "    target_names=label_encoder.classes_,\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=label_encoder.classes_,\n",
    "    yticklabels=label_encoder.classes_\n",
    ")\n",
    "plt.title('Matrice de Confusion - Intent Classification')\n",
    "plt.ylabel('Vraie Classe')\n",
    "plt.xlabel('Classe Pr√©dite')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Sauvegarder le meilleur mod√®le\n",
    "intent_trainer.save_model('/content/drive/MyDrive/nlp_dataset/models/intent_classifier_best')\n",
    "tokenizer.save_pretrained('/content/drive/MyDrive/nlp_dataset/models/intent_classifier_best')\n",
    "\n",
    "print(\"\\nMod√®le sauvegard√© dans Google Drive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b6c7fb",
   "metadata": {},
   "source": [
    "## 7 - Pr√©paration du Dataset NER (Token Classification)\n",
    "\n",
    "Conversion des annotations au format **BIO** (Begin-Inside-Outside) pour le NER :\n",
    "- **O** : Pas une entit√©\n",
    "- **B-Departure** : D√©but du lieu de d√©part\n",
    "- **I-Departure** : Continuation du lieu de d√©part\n",
    "- **B-Destination** : D√©but du lieu de destination\n",
    "- **I-Destination** : Continuation du lieu de destination\n",
    "\n",
    "Seuls les exemples avec `intent=TRIP` ont des annotations NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b23c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Tokenizer rapide pour NER (n√©cessaire pour offset_mapping)\n",
    "tokenizer_fast = AutoTokenizer.from_pretrained('camembert-base', use_fast=True)\n",
    "\n",
    "# Labels NER en format BIO\n",
    "ner_labels = ['O', 'B-Departure', 'I-Departure', 'B-Destination', 'I-Destination']\n",
    "label2id = {label: i for i, label in enumerate(ner_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(\"Labels NER:\")\n",
    "for label_id, label_name in id2label.items():\n",
    "    print(f\"   {label_id}: {label_name}\")\n",
    "\n",
    "def convert_to_bio_tags(text, entities):\n",
    "    \"\"\"\n",
    "    Convertit les annotations (start, end, label) en tags BIO pour chaque token.\n",
    "    \"\"\"\n",
    "    # Tokenizer le texte\n",
    "    encoding = tokenizer_fast(\n",
    "        text,\n",
    "        return_offsets_mapping=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    offset_mapping = encoding['offset_mapping']\n",
    "    labels = ['O'] * len(offset_mapping)\n",
    "    \n",
    "    # Pour chaque entit√©, marquer les tokens correspondants\n",
    "    for entity in entities:\n",
    "        start_char = entity['start']\n",
    "        end_char = entity['end']\n",
    "        entity_label = entity['label']\n",
    "        \n",
    "        token_indices = []\n",
    "        for idx, (token_start, token_end) in enumerate(offset_mapping):\n",
    "            # Ignorer les tokens padding\n",
    "            if token_start == token_end == 0:\n",
    "                continue\n",
    "            \n",
    "            # V√©rifier si le token chevauche l'entit√©\n",
    "            if not (token_end <= start_char or token_start >= end_char):\n",
    "                token_indices.append(idx)\n",
    "        \n",
    "        # Appliquer les tags BIO\n",
    "        if token_indices:\n",
    "            labels[token_indices[0]] = f'B-{entity_label}'\n",
    "            for idx in token_indices[1:]:\n",
    "                labels[idx] = f'I-{entity_label}'\n",
    "    \n",
    "    # Convertir en IDs\n",
    "    label_ids = [label2id.get(label, 0) for label in labels]\n",
    "    \n",
    "    return {\n",
    "        'input_ids': encoding['input_ids'],\n",
    "        'attention_mask': encoding['attention_mask'],\n",
    "        'labels': label_ids\n",
    "    }\n",
    "\n",
    "# Pr√©parer les datasets NER\n",
    "print(\"\\nConversion en format BIO...\")\n",
    "ner_train_data = []\n",
    "for _, row in train_df.iterrows():\n",
    "    entities = row['parsed_entities'] if row['intent'] == 'TRIP' else []\n",
    "    ner_example = convert_to_bio_tags(row['text'], entities)\n",
    "    ner_train_data.append(ner_example)\n",
    "\n",
    "ner_test_data = []\n",
    "for _, row in test_df.iterrows():\n",
    "    entities = row['parsed_entities'] if row['intent'] == 'TRIP' else []\n",
    "    ner_example = convert_to_bio_tags(row['text'], entities)\n",
    "    ner_test_data.append(ner_example)\n",
    "\n",
    "# Cr√©er les datasets HuggingFace\n",
    "ner_train_dataset = Dataset.from_list(ner_train_data)\n",
    "ner_test_dataset = Dataset.from_list(ner_test_data)\n",
    "\n",
    "print(f\"Datasets NER cr√©√©s:\")\n",
    "print(f\"   Train: {len(ner_train_dataset)} exemples\")\n",
    "print(f\"   Test: {len(ner_test_dataset)} exemples\")\n",
    "\n",
    "# Afficher un exemple\n",
    "print(f\"\\nExemple de tokenization BIO:\")\n",
    "example_idx = train_df[train_df['intent'] == 'TRIP'].index[0]\n",
    "example_text = train_df.loc[example_idx, 'text']\n",
    "example_entities = train_df.loc[example_idx, 'parsed_entities']\n",
    "print(f\"Texte: {example_text}\")\n",
    "print(f\"Entities: {example_entities}\")\n",
    "\n",
    "tokens = tokenizer_fast.tokenize(example_text)\n",
    "bio_example = convert_to_bio_tags(example_text, example_entities)\n",
    "bio_labels = [id2label[lid] for lid in bio_example['labels']]\n",
    "\n",
    "print(f\"\\nTokens et labels BIO (premiers 15):\")\n",
    "for token, label in list(zip(tokens, bio_labels))[:15]:\n",
    "    print(f\"   {token:20s} ‚Üí {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bea529",
   "metadata": {},
   "source": [
    "## 8 - Fine-Tuning NER avec CamemBERT (Token Classification)\n",
    "\n",
    "Entra√Ænement d'un mod√®le de **Named Entity Recognition** bas√© sur CamemBERT.\n",
    "\n",
    "**T√¢che** : Identifier et extraire les lieux de d√©part (Departure) et de destination (Destination) dans les phrases de voyage.\n",
    "\n",
    "**Param√®tres** :\n",
    "- Mod√®le : `camembert-base` (Token Classification)\n",
    "- Nombre d'epochs : 4\n",
    "- Batch size : 8 (train) / 16 (eval)\n",
    "- Learning rate : 3e-5\n",
    "- M√©trique : F1-Score (seqeval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e0589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Charger le mod√®le CamemBERT pour Token Classification\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    'camembert-base',\n",
    "    num_labels=len(ner_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Data collator pour padding dynamique\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer_fast)\n",
    "\n",
    "# Charger la m√©trique seqeval\n",
    "seqeval_metric = evaluate.load('seqeval')\n",
    "\n",
    "def compute_ner_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Calcule les m√©triques NER (precision, recall, F1) avec seqeval.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Convertir les pr√©dictions et labels en listes de tags\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    for pred_seq, label_seq in zip(predictions, labels):\n",
    "        true_tags = []\n",
    "        pred_tags = []\n",
    "        \n",
    "        for pred_id, label_id in zip(pred_seq, label_seq):\n",
    "            # Ignorer les labels de padding (-100)\n",
    "            if label_id != -100:\n",
    "                true_tags.append(id2label[label_id])\n",
    "                pred_tags.append(id2label[pred_id])\n",
    "        \n",
    "        true_labels.append(true_tags)\n",
    "        pred_labels.append(pred_tags)\n",
    "    \n",
    "    # Calculer les m√©triques\n",
    "    results = seqeval_metric.compute(predictions=pred_labels, references=true_labels)\n",
    "    \n",
    "    return {\n",
    "        'precision': results['overall_precision'],\n",
    "        'recall': results['overall_recall'],\n",
    "        'f1': results['overall_f1'],\n",
    "        'accuracy': results['overall_accuracy']\n",
    "    }\n",
    "\n",
    "# Arguments d'entra√Ænement (optimis√©s pour GPU)\n",
    "ner_training_args = TrainingArguments(\n",
    "    output_dir='/content/drive/MyDrive/nlp_dataset/models/ner_model',\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    logging_steps=100,\n",
    "    warmup_steps=200,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    push_to_hub=False,\n",
    "    report_to='none',  # D√©sactiver WandB et autres trackers\n",
    ")\n",
    "\n",
    "# Cr√©er le Trainer\n",
    "ner_trainer = Trainer(\n",
    "    model=ner_model,\n",
    "    args=ner_training_args,\n",
    "    train_dataset=ner_train_dataset,\n",
    "    eval_dataset=ner_test_dataset,\n",
    "    tokenizer=tokenizer_fast,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_ner_metrics\n",
    ")\n",
    "\n",
    "print(\"üöÄ D√©but du fine-tuning NER (Token Classification)...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Entra√Æner le mod√®le\n",
    "ner_trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Fine-tuning NER termin√©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96f00ff",
   "metadata": {},
   "source": [
    "## 9 - √âvaluation du Mod√®le NER\n",
    "\n",
    "√âvalue le mod√®le NER sur le test set avec des m√©triques d√©taill√©es par entit√© :\n",
    "- **Precision** : Pr√©cision de d√©tection des entit√©s\n",
    "- **Recall** : Taux de rappel des entit√©s\n",
    "- **F1-Score** : Score F1 global et par type d'entit√© (Departure, Destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45470ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report as seqeval_report\n",
    "\n",
    "# √âvaluer sur le test set\n",
    "ner_eval_results = ner_trainer.evaluate()\n",
    "\n",
    "print(\"R√©sultats NER sur le Test Set:\")\n",
    "print(\"=\"*70)\n",
    "for key, value in ner_eval_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"   {key}: {value:.4f}\")\n",
    "\n",
    "# Pr√©dictions d√©taill√©es\n",
    "ner_predictions = ner_trainer.predict(ner_test_dataset)\n",
    "predicted_logits = ner_predictions.predictions\n",
    "predicted_labels = np.argmax(predicted_logits, axis=-1)\n",
    "true_labels = ner_predictions.label_ids\n",
    "\n",
    "# Convertir en tags pour seqeval\n",
    "true_tags_list = []\n",
    "pred_tags_list = []\n",
    "\n",
    "for pred_seq, label_seq in zip(predicted_labels, true_labels):\n",
    "    true_tags = []\n",
    "    pred_tags = []\n",
    "    \n",
    "    for pred_id, label_id in zip(pred_seq, label_seq):\n",
    "        if label_id != -100:\n",
    "            true_tags.append(id2label[label_id])\n",
    "            pred_tags.append(id2label[pred_id])\n",
    "    \n",
    "    true_tags_list.append(true_tags)\n",
    "    pred_tags_list.append(pred_tags)\n",
    "\n",
    "# Classification report d√©taill√©\n",
    "print(\"\\nClassification Report NER (par entit√©):\")\n",
    "print(\"=\"*70)\n",
    "print(seqeval_report(true_tags_list, pred_tags_list, digits=4))\n",
    "\n",
    "# Sauvegarder le meilleur mod√®le\n",
    "ner_trainer.save_model('/content/drive/MyDrive/nlp_dataset/models/ner_model_best')\n",
    "tokenizer_fast.save_pretrained('/content/drive/MyDrive/nlp_dataset/models/ner_model_best')\n",
    "\n",
    "print(\"\\nMod√®le NER sauvegard√© dans Google Drive!\")\n",
    "\n",
    "# Afficher quelques exemples de pr√©dictions\n",
    "print(\"\\nExemples de pr√©dictions NER:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "trip_test_indices = test_df[test_df['intent'] == 'TRIP'].head(5).index\n",
    "for idx in trip_test_indices:\n",
    "    text = test_df.loc[idx, 'text']\n",
    "    true_entities = test_df.loc[idx, 'parsed_entities']\n",
    "    \n",
    "    # Pr√©dire avec le mod√®le\n",
    "    inputs = tokenizer_fast(text, return_tensors='pt', truncation=True, max_length=128)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        ner_model.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = ner_model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    # D√©coder les tokens et labels\n",
    "    tokens = tokenizer_fast.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    predicted_tags = [id2label[p.item()] for p in predictions[0]]\n",
    "    \n",
    "    print(f\"\\nTexte: {text}\")\n",
    "    print(f\"Vraies entit√©s: {true_entities}\")\n",
    "    print(\"Pr√©dictions:\")\n",
    "    \n",
    "    current_entity = None\n",
    "    current_tokens = []\n",
    "    \n",
    "    for token, tag in zip(tokens, predicted_tags):\n",
    "        if token in ['<s>', '</s>', '<pad>']:\n",
    "            continue\n",
    "        \n",
    "        if tag.startswith('B-'):\n",
    "            if current_entity:\n",
    "                print(f\"   {current_entity}: {''.join(current_tokens).replace('‚ñÅ', ' ').strip()}\")\n",
    "            current_entity = tag[2:]\n",
    "            current_tokens = [token]\n",
    "        elif tag.startswith('I-') and current_entity:\n",
    "            current_tokens.append(token)\n",
    "        else:\n",
    "            if current_entity:\n",
    "                print(f\"   {current_entity}: {''.join(current_tokens).replace('‚ñÅ', ' ').strip()}\")\n",
    "                current_entity = None\n",
    "                current_tokens = []\n",
    "    \n",
    "    if current_entity:\n",
    "        print(f\"   {current_entity}: {''.join(current_tokens).replace('‚ñÅ', ' ').strip()}\")\n",
    "    \n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dbecb5",
   "metadata": {},
   "source": [
    "## 10 - Pipeline d'Inf√©rence Complet\n",
    "\n",
    "Combine les deux mod√®les (Intent Classification + NER) pour cr√©er un pipeline d'inf√©rence complet.\n",
    "\n",
    "**Workflow** :\n",
    "1. Pr√©dire l'intent du message\n",
    "2. Si intent = TRIP, extraire Departure et Destination avec NER\n",
    "3. Retourner le r√©sultat format√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888604ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Charger les pipelines\n",
    "print(\"Chargement des mod√®les...\")\n",
    "\n",
    "# Pipeline Intent Classification\n",
    "intent_pipeline = pipeline(\n",
    "    'text-classification',\n",
    "    model='/content/drive/MyDrive/nlp_dataset/models/intent_classifier_best',\n",
    "    tokenizer='/content/drive/MyDrive/nlp_dataset/models/intent_classifier_best',\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Pipeline NER\n",
    "ner_pipeline = pipeline(\n",
    "    'token-classification',\n",
    "    model='/content/drive/MyDrive/nlp_dataset/models/ner_model_best',\n",
    "    tokenizer='/content/drive/MyDrive/nlp_dataset/models/ner_model_best',\n",
    "    aggregation_strategy='simple',\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "print(\"Pipelines charg√©s!\")\n",
    "\n",
    "def predict_travel_order(text):\n",
    "    \"\"\"\n",
    "    Pipeline complet : Intent + NER\n",
    "    \"\"\"\n",
    "    # 1. Pr√©dire l'intent\n",
    "    intent_result = intent_pipeline(text)[0]\n",
    "    predicted_intent = intent_result['label']\n",
    "    confidence = intent_result['score']\n",
    "    \n",
    "    result = {\n",
    "        'text': text,\n",
    "        'intent': predicted_intent,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "    \n",
    "    # 2. Si TRIP, extraire les entit√©s\n",
    "    if predicted_intent == 'TRIP':\n",
    "        ner_results = ner_pipeline(text)\n",
    "        \n",
    "        departure = None\n",
    "        destination = None\n",
    "        \n",
    "        for entity in ner_results:\n",
    "            entity_label = entity.get('entity_group', entity.get('entity'))\n",
    "            word = entity['word'].replace('‚ñÅ', ' ').strip()\n",
    "            \n",
    "            if 'Departure' in entity_label and not departure:\n",
    "                departure = word\n",
    "            elif 'Destination' in entity_label and not destination:\n",
    "                destination = word\n",
    "        \n",
    "        result['departure'] = departure\n",
    "        result['destination'] = destination\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Tester sur quelques exemples\n",
    "print(\"\\nTests sur des exemples:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_examples = [\n",
    "    \"Je voudrais un billet de Cotonou √† Porto-Novo demain\",\n",
    "    \"Bonjour, un aller-retour Paris Marseille s'il vous pla√Æt\",\n",
    "    \"Merci pour votre email, je confirme ma pr√©sence\",\n",
    "    \"Hello, I need a ticket to London\",\n",
    "    \"zxqwerty azerty\"\n",
    "]\n",
    "\n",
    "for i, example in enumerate(test_examples, 1):\n",
    "    print(f\"\\n{i}. {example}\")\n",
    "    result = predict_travel_order(example)\n",
    "    print(f\"   Intent: {result['intent']} (confiance: {result['confidence']:.2%})\")\n",
    "    \n",
    "    if result['intent'] == 'TRIP':\n",
    "        dep = result.get('departure', 'N/A')\n",
    "        dest = result.get('destination', 'N/A')\n",
    "        print(f\"   Departure: {dep}\")\n",
    "        print(f\"   Destination: {dest}\")\n",
    "        print(f\"   ‚Üí Sortie: {i},{dep},{dest}\")\n",
    "    else:\n",
    "        print(f\"   ‚Üí Sortie: {i},{result['intent']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Pipeline d'inf√©rence pr√™t √† l'emploi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f327b0",
   "metadata": {},
   "source": [
    "## 11 - Traitement Batch d'un Fichier d'Input\n",
    "\n",
    "Lit un fichier d'input au format `sentenceID,sentence` et g√©n√®re un fichier de sortie avec les pr√©dictions.\n",
    "\n",
    "**Format d'entr√©e** : `1,Je voudrais un billet Paris Lyon`  \n",
    "**Format de sortie** :\n",
    "- Si TRIP : `1,Paris,Lyon`\n",
    "- Sinon : `1,NOT_TRIP` (ou UNKNOWN, NOT_FRENCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0408ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch_file(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Traite un fichier batch et g√©n√®re les pr√©dictions.\n",
    "    \n",
    "    Format entr√©e: sentenceID,sentence\n",
    "    Format sortie: sentenceID,departure,destination (si TRIP) ou sentenceID,intent (sinon)\n",
    "    \"\"\"\n",
    "    output_lines = []\n",
    "    \n",
    "    print(f\"Lecture de {input_file}...\")\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    print(f\"Traitement de {len(lines)} phrases...\")\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Parser la ligne\n",
    "            parts = line.split(',', 1)\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            \n",
    "            sentence_id, text = parts\n",
    "            \n",
    "            # Pr√©dire\n",
    "            result = predict_travel_order(text)\n",
    "            \n",
    "            # Formater la sortie\n",
    "            if result['intent'] == 'TRIP':\n",
    "                departure = result.get('departure', 'UNKNOWN')\n",
    "                destination = result.get('destination', 'UNKNOWN')\n",
    "                \n",
    "                if departure and destination:\n",
    "                    output_line = f\"{sentence_id},{departure},{destination}\"\n",
    "                else:\n",
    "                    output_line = f\"{sentence_id},UNKNOWN\"\n",
    "            else:\n",
    "                output_line = f\"{sentence_id},{result['intent']}\"\n",
    "            \n",
    "            output_lines.append(output_line)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur sur la ligne: {line[:50]}... ‚Üí {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Sauvegarder\n",
    "    print(f\"\\nSauvegarde de {len(output_lines)} r√©sultats dans {output_file}...\")\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(output_lines))\n",
    "    \n",
    "    print(f\"Traitement termin√©!\")\n",
    "    return output_file\n",
    "\n",
    "# Exemple d'utilisation\n",
    "input_file_path = '/content/drive/MyDrive/nlp_dataset/sample_nlp_input.txt'\n",
    "output_file_path = '/content/drive/MyDrive/nlp_dataset/predictions_output.txt'\n",
    "\n",
    "# V√©rifier si le fichier existe\n",
    "if os.path.exists(input_file_path):\n",
    "    result_file = process_batch_file(input_file_path, output_file_path)\n",
    "    \n",
    "    print(f\"\\nAper√ßu des r√©sultats:\")\n",
    "    print(\"=\"*70)\n",
    "    with open(result_file, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 10:  # Afficher les 10 premi√®res lignes\n",
    "                break\n",
    "            print(line.strip())\n",
    "    \n",
    "    print(\"\\nFichier complet sauvegard√© dans Google Drive!\")\n",
    "else:\n",
    "    print(f\"Fichier d'entr√©e non trouv√©: {input_file_path}\")\n",
    "    print(\"   Uploadez votre fichier dans Google Drive ou modifiez le chemin.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34ff131",
   "metadata": {},
   "source": [
    "## R√©capitulatif et Instructions d'Utilisation\n",
    "\n",
    "### Ce qui a √©t√© fait :\n",
    "1. **Fine-tuning Intent Classification** : Mod√®le CamemBERT entra√Æn√© pour classifier 4 types d'intent (TRIP, NOT_TRIP, UNKNOWN, NOT_FRENCH)\n",
    "2. **Fine-tuning NER** : Mod√®le CamemBERT entra√Æn√© pour extraire Departure et Destination\n",
    "3. **Pipeline d'inf√©rence** : Syst√®me complet combinant les deux mod√®les\n",
    "4. **Traitement batch** : Fonction pour traiter des fichiers d'input en masse\n",
    "\n",
    "### Mod√®les sauvegard√©s dans Google Drive :\n",
    "- `/content/drive/MyDrive/nlp_dataset/models/intent_classifier_best/`\n",
    "- `/content/drive/MyDrive/nlp_dataset/models/ner_model_best/`\n",
    "\n",
    "### Pour r√©utiliser les mod√®les plus tard :\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Charger le pipeline Intent\n",
    "intent_pipe = pipeline(\n",
    "    'text-classification',\n",
    "    model='/content/drive/MyDrive/nlp_dataset/models/intent_classifier_best',\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Charger le pipeline NER\n",
    "ner_pipe = pipeline(\n",
    "    'token-classification',\n",
    "    model='/content/drive/MyDrive/nlp_dataset/models/ner_model_best',\n",
    "    aggregation_strategy='simple',\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Utiliser\n",
    "result = intent_pipe(\"Je veux aller de Cotonou √† Porto-Novo\")\n",
    "entities = ner_pipe(\"Je veux aller de Cotonou √† Porto-Novo\")\n",
    "```\n",
    "\n",
    "### Performances attendues :\n",
    "- **Intent Classification** : F1-Score > 0.95 (selon la qualit√© du dataset)\n",
    "- **NER** : F1-Score > 0.90 pour Departure et Destination\n",
    "\n",
    "### Optimisations possibles :\n",
    "1. Augmenter le nombre d'epochs pour am√©liorer les performances\n",
    "2. Ajuster les hyperparam√®tres (learning rate, batch size, weight decay)\n",
    "3. Essayer d'autres mod√®les fran√ßais : FlauBERT, BARThez\n",
    "4. Ajouter de l'augmentation de donn√©es\n",
    "5. Utiliser des techniques d'ensemble (combiner plusieurs mod√®les)\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook compl√©t√© avec succ√®s !**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
