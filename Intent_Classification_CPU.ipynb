{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b37c4f5",
   "metadata": {},
   "source": [
    "# Travel Order Resolver — Notebook\n",
    "This notebook implements an intent classifier + NER (Departure/Destination) for the **Travel Order Resolver** project.\n",
    "It uses **CamemBERT** (transfer learning) via Hugging Face Transformers.\n",
    "\n",
    "**What is included**\n",
    "- Installation of dependencies\n",
    "- Loading dataset (from Google Drive or upload)\n",
    "- Preprocessing and tokenization\n",
    "- Fine-tuning (intent classification) and token-classification (NER)\n",
    "- Inference pipeline that reads `sentenceID,sentence` and writes outputs in the required format\n",
    "\n",
    "**Important notes**\n",
    "- The GPU notebook is optimized to run on Google Colab with a GPU runtime.\n",
    "- The CPU notebook is lighter and intended for local execution without a GPU (slower).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## CPU notebook (local execution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa840a6",
   "metadata": {},
   "source": [
    "This cell verifies whether required dependencies are already installed in the current Python environment (for example a `venv`). If any package is missing it installs packages from `requirements.txt` and ensures a CPU-only build of `torch` is installed. Using the current Python executable (`sys.executable`) avoids modifying a different environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4c2a21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: /home/stanley-honkpehedji/Téléchargements/nlp_miniproject/.venv/bin/python\n",
      "All listed Python packages are already installed.\n",
      "torch is installed (version:  2.9.0+cpu )\n",
      "Dependencies ensured for CPU run.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "\n",
    "# Try to import key packages to avoid unnecessary reinstalls\n",
    "missing = []\n",
    "for pkg in (\"transformers\",\"datasets\",\"evaluate\",\"seqeval\",\"tokenizers\",\"sacrebleu\"):\n",
    "    try:\n",
    "        __import__(pkg)\n",
    "    except Exception:\n",
    "        missing.append(pkg)\n",
    "\n",
    "if missing:\n",
    "    print(\"Missing packages detected:\", missing)\n",
    "    print(\"Installing from requirements.txt using the current Python environment...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"requirements.txt\"])\n",
    "else:\n",
    "    print(\"All listed Python packages are already installed.\")\n",
    "\n",
    "# Ensure CPU-only torch is present (install if missing)\n",
    "try:\n",
    "    import torch\n",
    "    print(\"torch is installed (version: \", torch.__version__, \")\")\n",
    "except Exception:\n",
    "    print(\"Installing CPU-only torch from PyTorch index...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"torch\", \"--index-url\", \"https://download.pytorch.org/whl/cpu\"]) \n",
    "\n",
    "print(\"Dependencies ensured for CPU run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3f3756",
   "metadata": {},
   "source": [
    "Cette cellule charge les fichiers CSV d'entraînement et de test en DataFrames pandas. Si les fichiers ne sont pas trouvés dans `/content/`, ajustez les chemins vers votre dossier local ou Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a47fb1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (7724, 3) Test shape: (1827, 3)\n",
      "Data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load dataset (adjust paths as needed)\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Paths relative to the notebook's directory\n",
    "train_csv = \"dataset/train_set.csv\"\n",
    "test_csv = \"dataset/test_set.csv\"\n",
    "\n",
    "# If using Drive, change to the drive path (example shown in previous cell)\n",
    "if not os.path.exists(train_csv) or not os.path.exists(test_csv):\n",
    "    print(f\"Train or test CSV not found.\")\n",
    "    print(f\"Looking for: {os.path.abspath(train_csv)}\")\n",
    "    print(f\"Looking for: {os.path.abspath(test_csv)}\")\n",
    "    print(\"Please adjust the paths to match your file locations.\")\n",
    "else:\n",
    "    train_df = pd.read_csv(train_csv, encoding=\"utf-8\")\n",
    "    test_df = pd.read_csv(test_csv, encoding=\"utf-8\")\n",
    "    print(\"Train shape:\", train_df.shape, \"Test shape:\", test_df.shape)\n",
    "    print(\"Data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edd20ed",
   "metadata": {},
   "source": [
    "Charger les données : lire `train_set.csv` et `test_set.csv` en utilisant pandas. Le code suivant essaie `/content/` par défaut et affiche la forme des DataFrames si trouvés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0431c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed entities (example):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>intent</th>\n",
       "      <th>entities</th>\n",
       "      <th>parsed_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Comment aller de Grenoble à Nogent-le-Rotrou ?</td>\n",
       "      <td>TRIP</td>\n",
       "      <td>[{\"start\": 17, \"end\": 25, \"label\": \"Departure\"...</td>\n",
       "      <td>[{'start': 17, 'end': 25, 'label': 'Departure'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bonsoir, je veux un voyage de Gex vers Lille.</td>\n",
       "      <td>TRIP</td>\n",
       "      <td>[{\"start\": 30, \"end\": 33, \"label\": \"Departure\"...</td>\n",
       "      <td>[{'start': 30, 'end': 33, 'label': 'Departure'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nous vous suis reconnaissant pour votre collab...</td>\n",
       "      <td>NOT_TRIP</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    intent  \\\n",
       "0     Comment aller de Grenoble à Nogent-le-Rotrou ?      TRIP   \n",
       "1      Bonsoir, je veux un voyage de Gex vers Lille.      TRIP   \n",
       "2  Nous vous suis reconnaissant pour votre collab...  NOT_TRIP   \n",
       "\n",
       "                                            entities  \\\n",
       "0  [{\"start\": 17, \"end\": 25, \"label\": \"Departure\"...   \n",
       "1  [{\"start\": 30, \"end\": 33, \"label\": \"Departure\"...   \n",
       "2                                                 []   \n",
       "\n",
       "                                     parsed_entities  \n",
       "0  [{'start': 17, 'end': 25, 'label': 'Departure'...  \n",
       "1  [{'start': 30, 'end': 33, 'label': 'Departure'...  \n",
       "2                                                 []  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Inspect and parse 'entities' JSON field (if present)\n",
    "import json\n",
    "def parse_entities_field(row):\n",
    "    try:\n",
    "        ents = json.loads(row['entities'])\n",
    "    except Exception:\n",
    "        ents = []\n",
    "    valid = []\n",
    "    for ent in ents:\n",
    "        if 'start' in ent and 'end' in ent and 0 <= ent['start'] < ent['end'] <= len(row['text']):\n",
    "            valid.append(ent)\n",
    "    return valid\n",
    "\n",
    "train_df['parsed_entities'] = train_df.apply(parse_entities_field, axis=1)\n",
    "test_df['parsed_entities'] = test_df.apply(parse_entities_field, axis=1)\n",
    "print('Parsed entities (example):')\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e505a114",
   "metadata": {},
   "source": [
    "Cette cellule parse et valide le champ JSON `entities` (s'il existe) pour chaque ligne, et construit une colonne `parsed_entities` contenant les spans valides (start/end)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8fdae9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent classes: ['NOT_FRENCH', 'NOT_TRIP', 'TRIP', 'UNKNOWN']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7724/7724 [00:01<00:00, 7402.80 examples/s]\n",
      "Map: 100%|██████████| 7724/7724 [00:01<00:00, 7402.80 examples/s]\n",
      "Map: 100%|██████████| 1827/1827 [00:00<00:00, 9042.27 examples/s]\n",
      "\n",
      "Map: 100%|██████████| 7724/7724 [00:00<00:00, 14818.69 examples/s]\n",
      "Map: 100%|██████████| 7724/7724 [00:00<00:00, 14818.69 examples/s]\n",
      "Map: 100%|██████████| 1827/1827 [00:00<00:00, 14754.94 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor(2), 'input_ids': tensor([    5,   841,   632,     8,  7069,    15, 29291,    26,   185,    26,\n",
      "         5952, 21149,   106,     6,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare Hugging Face datasets for intent classification\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_df['intent'].unique())\n",
    "num_labels = len(label_encoder.classes_)\n",
    "print(\"Intent classes:\", list(label_encoder.classes_))\n",
    "\n",
    "hf_train = Dataset.from_pandas(train_df[['text','intent']].rename(columns={'intent':'label'}))\n",
    "hf_test  = Dataset.from_pandas(test_df[['text','intent']].rename(columns={'intent':'label'}))\n",
    "\n",
    "def encode_label(example):\n",
    "    example['label'] = int(label_encoder.transform([example['label']])[0])\n",
    "    return example\n",
    "\n",
    "hf_train = hf_train.map(encode_label)\n",
    "hf_test = hf_test.map(encode_label)\n",
    "\n",
    "model_name = \"camembert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_classification(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "hf_train = hf_train.map(tokenize_classification, batched=True, remove_columns=['text'])\n",
    "hf_test = hf_test.map(tokenize_classification, batched=True, remove_columns=['text'])\n",
    "\n",
    "hf_train = hf_train.rename_column(\"label\", \"labels\")\n",
    "hf_test = hf_test.rename_column(\"label\", \"labels\")\n",
    "hf_train.set_format(type=\"torch\")\n",
    "hf_test.set_format(type=\"torch\")\n",
    "\n",
    "print(hf_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6e2afa",
   "metadata": {},
   "source": [
    "Cette cellule convertit les DataFrames en datasets Hugging Face pour la classification d'intent : encodage des labels, tokenization (padding/truncation) et formatage pour PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fd0b62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m     acc = (preds == labels).mean()\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m: acc, \u001b[33m\"\u001b[39m\u001b[33mf1_macro\u001b[39m\u001b[33m\"\u001b[39m: f1}\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./camembert-intent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mf1_macro\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgreater_is_better\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     27\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m trainer = Trainer(\n\u001b[32m     30\u001b[39m     model=model_cls,\n\u001b[32m     31\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     tokenizer=tokenizer\n\u001b[32m     36\u001b[39m )\n\u001b[32m     38\u001b[39m trainer.train()\n",
      "\u001b[31mTypeError\u001b[39m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fine-tune CamemBERT for Intent Classification (CPU-friendly args)\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import evaluate, numpy as np\n",
    "\n",
    "model_cls = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics_intent(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    f1 = metric.compute(predictions=preds, references=labels, average=\"macro\")['f1']\n",
    "    acc = (preds == labels).mean()\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./camembert-intent\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    fp16=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_cls,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_test,\n",
    "    compute_metrics=compute_metrics_intent,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./camembert-intent-best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf54375",
   "metadata": {},
   "source": [
    "Cette cellule effectue le fine-tuning de CamemBERT pour la classification d'intent en utilisant la classe Trainer de Hugging Face. Les arguments sont adaptés pour un environnement CPU (lot réduit, fp16 désactivé)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8f76df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare NER dataset (token-classification) - convert spans to BIO using fast tokenizer offsets\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer_fast = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "ner_labels = ['O', 'B-Departure', 'I-Departure', 'B-Destination', 'I-Destination']\n",
    "label2id = {l:i for i,l in enumerate(ner_labels)}\n",
    "id2label = {i:l for l,i in label2id.items()}\n",
    "\n",
    "def spans_to_bio(text, spans):\n",
    "    encoding = tokenizer_fast(text, return_offsets_mapping=True, truncation=True, max_length=128)\n",
    "    offsets = encoding['offset_mapping']\n",
    "    labels = ['O'] * len(offsets)\n",
    "    for ent in spans:\n",
    "        st, ed = ent['start'], ent['end']\n",
    "        token_indices = []\n",
    "        for i,(a,b) in enumerate(offsets):\n",
    "            if a==b==0:\n",
    "                continue\n",
    "            if not (b <= st or a >= ed):\n",
    "                token_indices.append(i)\n",
    "        if not token_indices:\n",
    "            continue\n",
    "        labels[token_indices[0]] = 'B-' + ent['label']\n",
    "        for idx in token_indices[1:]:\n",
    "            labels[idx] = 'I-' + ent['label']\n",
    "    label_ids = [label2id.get(l,0) for l in labels]\n",
    "    return encoding, label_ids\n",
    "\n",
    "# Build HF datasets (train/test)\n",
    "from datasets import Dataset\n",
    "ner_rows = []\n",
    "for _, r in train_df.iterrows():\n",
    "    text = r['text']\n",
    "    spans = r['parsed_entities'] if r['intent']=='TRIP' else []\n",
    "    _, label_ids = spans_to_bio(text, spans)\n",
    "    ner_rows.append({'text': text, 'labels': label_ids})\n",
    "\n",
    "ner_train_ds = Dataset.from_list(ner_rows)\n",
    "\n",
    "ner_rows_test = []\n",
    "for _, r in test_df.iterrows():\n",
    "    text = r['text']\n",
    "    spans = r['parsed_entities'] if r['intent']=='TRIP' else []\n",
    "    _, label_ids = spans_to_bio(text, spans)\n",
    "    ner_rows_test.append({'text': text, 'labels': label_ids})\n",
    "\n",
    "ner_test_ds = Dataset.from_list(ner_rows_test)\n",
    "print('NER datasets prepared (counts):', len(ner_train_ds), len(ner_test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089ce065",
   "metadata": {},
   "source": [
    "Préparer les données pour la tâche NER (token-classification). Le code ci-dessous convertit les spans en étiquettes BIO alignées avec les tokens du tokenizer rapide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d604ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fine-tune CamemBERT for NER (CPU-friendly)\n",
    "from transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    "import numpy as np, seqeval.metrics as seq_metrics\n",
    "\n",
    "model_ner = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(ner_labels), id2label=id2label, label2id=label2id)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer_fast)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer_fast(examples['text'], truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
    "    return {'input_ids': tokenized_inputs['input_ids'].tolist(), 'attention_mask': tokenized_inputs['attention_mask'].tolist(), 'labels': examples['labels']}\n",
    "\n",
    "ner_train_tok = ner_train_ds.map(lambda x: tokenize_and_align_labels(x), batched=True)\n",
    "ner_test_tok = ner_test_ds.map(lambda x: tokenize_and_align_labels(x), batched=True)\n",
    "\n",
    "ner_train_tok.set_format(type='torch', columns=['input_ids','attention_mask','labels'])\n",
    "ner_test_tok.set_format(type='torch', columns=['input_ids','attention_mask','labels'])\n",
    "\n",
    "def align_preds(predictions, label_ids):\n",
    "    preds = np.argmax(predictions, axis=-1)\n",
    "    preds_list = [[id2label[p] for p in pred] for pred in preds]\n",
    "    labels_list = [[id2label[l] for l in lab] for lab in label_ids]\n",
    "    return preds_list, labels_list\n",
    "\n",
    "def compute_metrics_ner(p):\n",
    "    preds, labels = p\n",
    "    preds_list, labels_list = align_preds(preds, labels)\n",
    "    return {\n",
    "        'precision': seq_metrics.precision_score(labels_list, preds_list),\n",
    "        'recall': seq_metrics.recall_score(labels_list, preds_list),\n",
    "        'f1': seq_metrics.f1_score(labels_list, preds_list)\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./camembert-ner',\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=4,\n",
    "    evaluation_strategy='epoch',\n",
    "    num_train_epochs=2,\n",
    "    save_strategy='epoch',\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "trainer_ner = Trainer(\n",
    "    model=model_ner,\n",
    "    args=training_args,\n",
    "    train_dataset=ner_train_tok,\n",
    "    eval_dataset=ner_test_tok,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer_fast,\n",
    "    compute_metrics=compute_metrics_ner\n",
    ")\n",
    "\n",
    "trainer_ner.train()\n",
    "trainer_ner.save_model('./camembert-ner-best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e8bc02",
   "metadata": {},
   "source": [
    "Cette cellule fine-tune CamemBERT pour la tâche de token-classification (NER). Les métriques sont calculées avec `seqeval` et les arguments d'entraînement sont configurés pour CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51485f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inference pipeline (load saved models and run on new lines)\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "# Load intent model\n",
    "intent_tokenizer = AutoTokenizer.from_pretrained('./camembert-intent-best')\n",
    "intent_model = AutoModelForSequenceClassification.from_pretrained('./camembert-intent-best')\n",
    "\n",
    "# Load ner model\n",
    "ner_tokenizer = AutoTokenizer.from_pretrained('./camembert-ner-best', use_fast=True)\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained('./camembert-ner-best')\n",
    "\n",
    "intent_pipe = pipeline('text-classification', model=intent_model, tokenizer=intent_tokenizer)\n",
    "ner_pipe = pipeline('token-classification', model=ner_model, tokenizer=ner_tokenizer, aggregation_strategy='simple')\n",
    "\n",
    "# Example inference function\n",
    "def predict_line(sentenceID, sentence):\n",
    "    # intent\n",
    "    inputs = intent_tokenizer(sentence, return_tensors='pt', truncation=True, max_length=128)\n",
    "    logits = intent_model(**inputs).logits.detach().cpu().numpy()[0]\n",
    "    pred_id = int(logits.argmax())\n",
    "    print('Pred intent id:', pred_id)\n",
    "    # NER\n",
    "    ner_res = ner_pipe(sentence)\n",
    "    print('NER result:', ner_res)\n",
    "    return None\n",
    "\n",
    "# Example usage\n",
    "print(predict_line('1', 'Je voudrais un billet Toulouse Paris.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3797b9f8",
   "metadata": {},
   "source": [
    "Cette cellule met en place le pipeline d'inférence : elle recharge les modèles sauvegardés (intent + ner) et fournit une fonction `predict_line` qui retourne l'intent et les entités pour une phrase donnée."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09be3873",
   "metadata": {},
   "source": [
    "\n",
    "# Final notes\n",
    "- The GPU notebook is for Colab with a GPU runtime (recommended). Use Runtime -> Change runtime type -> GPU.\n",
    "- The CPU notebook is slower; training on a CPU may take a long time.\n",
    "- Save your models in the drive if you want to keep them across sessions (example path: /content/drive/MyDrive/nlp_miniprojects/).\n",
    "- Save the tokenizer and label mappings alongside the model for correct inference later.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
